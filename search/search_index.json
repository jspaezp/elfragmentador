{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ElFragmentador ElFragmentador This repository attempts to implement a neural net that leverages the transformer architecture to predict peptide properties (retention time and fragmentation). Usage Please check out The Quickstart guide for usage instructions. Why transformers? Because we can... Just kidding The transformer architecture provides several benefits over the standard approach on fragment prediction (LSTM/RNN). On the training side it allows the parallel computation of whole sequences, whilst in LSTMs one element has to be passed at a time. In addition it gives the model itself a better chance to study the direct interactions between the elements that are being passed. On the other hand, it allows a much better interpretability of the model, since the 'self-attention' can be visualized on the input and in that way see what the model is focusing on while generating the prediction. Inspiration for this project Many of the elements from this project are actually a combination of the principles shown in the Prosit paper and the Skyline poster on some of the elements to encode the peptides and the output fragment ions. On the transformer side of things I must admit that many of the elements of this project are derived from DETR: End to end detection using transformers in particular the trainable embeddings as an input for the decoder and some of the concepts discussed about it on Yannic Kilcher's Youtube channel (which I highly recommend). Why the name? Two main reasons ... it translates to 'The fragmenter' in spanish and the project intends to predict fragmentation. On the other hand ... The name was free in pypi. Resources on transformers An amazing illustrated guide to understand the transformer architecture: http://jalammar.github.io/illustrated-transformer/ Full implementation of a transformer in pytorch with the explanation of each part: https://nlp.seas.harvard.edu/2018/04/03/attention.html Official pytorch implementation of the transformer: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html How fast is it? You can check how fast the model is in you specific system. Right now the CLI tests the speed only on CPU (the model can be run in GPU). Here I will predict the fasta file for SARS-COV2 poetry run elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 20 --out foo.dlib ... 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1701/1721 [00:14<00:00, 118.30it/s] ... ~100 predictions per second including pre-post processing and writting the enciclopeDIA library. On a GPU it is closer to ~1000 preds/sec How big is it? I have explored many variations on the model but currently the one distributed is only ~4mb. Models up to 200mb have been tried and they don't really give a big improvement in performance. \"Common\" questions What scale are the retention times predicted. Out of the model it uses a scaled version of the Biognosys retention time scale, so if using the base model, you will need to multiply by 100 and then you will get something compatible with the iRT kit. Is it any good? Well ... yes but if you want to see if it is good for you own data I have added an API to test the model on a spectral library (made with spectrast). Just get a checkpoint of the model, run the command: elfragmentador_evaluate {your_checkpoint.ckpt} {your_splib.sptxt} TODO add some benchmarking metrics to this readme ... Crosslinked peptides? No ETD ? No CID ? No Glycosilation ? No Negative Mode ? No No ? Not really ... I think all of those are interesting questions but AS IT IS RIGHT NOW it is not within the scope of the project. If you want to discuss it, write an issue in the repo and we can see if it is feasible. Acknowledgements Purdue Univ for the computational resources for the preparation of the data (Brown Cluster). Pytorch Lightning Team ... without this being open sourced this would not be posible. Weights and Biases (same as above).","title":"Home"},{"location":"#elfragmentador","text":"","title":"ElFragmentador"},{"location":"#elfragmentador_1","text":"This repository attempts to implement a neural net that leverages the transformer architecture to predict peptide properties (retention time and fragmentation).","title":"ElFragmentador"},{"location":"#usage","text":"Please check out The Quickstart guide for usage instructions.","title":"Usage"},{"location":"#why-transformers","text":"Because we can... Just kidding The transformer architecture provides several benefits over the standard approach on fragment prediction (LSTM/RNN). On the training side it allows the parallel computation of whole sequences, whilst in LSTMs one element has to be passed at a time. In addition it gives the model itself a better chance to study the direct interactions between the elements that are being passed. On the other hand, it allows a much better interpretability of the model, since the 'self-attention' can be visualized on the input and in that way see what the model is focusing on while generating the prediction.","title":"Why transformers?"},{"location":"#inspiration-for-this-project","text":"Many of the elements from this project are actually a combination of the principles shown in the Prosit paper and the Skyline poster on some of the elements to encode the peptides and the output fragment ions. On the transformer side of things I must admit that many of the elements of this project are derived from DETR: End to end detection using transformers in particular the trainable embeddings as an input for the decoder and some of the concepts discussed about it on Yannic Kilcher's Youtube channel (which I highly recommend).","title":"Inspiration for this project"},{"location":"#why-the-name","text":"Two main reasons ... it translates to 'The fragmenter' in spanish and the project intends to predict fragmentation. On the other hand ... The name was free in pypi.","title":"Why the name?"},{"location":"#resources-on-transformers","text":"An amazing illustrated guide to understand the transformer architecture: http://jalammar.github.io/illustrated-transformer/ Full implementation of a transformer in pytorch with the explanation of each part: https://nlp.seas.harvard.edu/2018/04/03/attention.html Official pytorch implementation of the transformer: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html","title":"Resources on transformers"},{"location":"#how-fast-is-it","text":"You can check how fast the model is in you specific system. Right now the CLI tests the speed only on CPU (the model can be run in GPU). Here I will predict the fasta file for SARS-COV2 poetry run elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 20 --out foo.dlib ... 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1701/1721 [00:14<00:00, 118.30it/s] ... ~100 predictions per second including pre-post processing and writting the enciclopeDIA library. On a GPU it is closer to ~1000 preds/sec","title":"How fast is it?"},{"location":"#how-big-is-it","text":"I have explored many variations on the model but currently the one distributed is only ~4mb. Models up to 200mb have been tried and they don't really give a big improvement in performance.","title":"How big is it?"},{"location":"#common-questions","text":"What scale are the retention times predicted. Out of the model it uses a scaled version of the Biognosys retention time scale, so if using the base model, you will need to multiply by 100 and then you will get something compatible with the iRT kit. Is it any good? Well ... yes but if you want to see if it is good for you own data I have added an API to test the model on a spectral library (made with spectrast). Just get a checkpoint of the model, run the command: elfragmentador_evaluate {your_checkpoint.ckpt} {your_splib.sptxt} TODO add some benchmarking metrics to this readme ... Crosslinked peptides? No ETD ? No CID ? No Glycosilation ? No Negative Mode ? No No ? Not really ... I think all of those are interesting questions but AS IT IS RIGHT NOW it is not within the scope of the project. If you want to discuss it, write an issue in the repo and we can see if it is feasible.","title":"\"Common\" questions"},{"location":"#acknowledgements","text":"Purdue Univ for the computational resources for the preparation of the data (Brown Cluster). Pytorch Lightning Team ... without this being open sourced this would not be posible. Weights and Biases (same as above).","title":"Acknowledgements"},{"location":"quickstart/","text":"Quickstart For a more detailed overview of how to use the command line, check the CLI Reference section of the documentation If you woul like any help/would like something implemented, feel free to open a github issue or message me :) Installation This is currently being distributed as a pypi package, to get the latest version use the following ... User Install: pip install elfragmentador Development install: git clone https://github.com/jspaezp/elfragmentador.git cd elfragmentador pip install /content/elfragmentador # or ... poetry install Usage Prediction $ elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 7 --out foo.dlib Rescoring This will add a couple of columns to a percolator input file, which can be used before you run it though percolator or mokapot! elfragmentador append_pin --pin { input .pin } --out { output .pin } --nce 30 .0 Check performance I have implemented a way to compare the predictions of the model with an several spectral librariy-related files. The easiest one for is the mokapot .peptides.txt file after a comet search. $ poetry run elfragmentador evaluate --input mokapot.peptides.txt.evaluation.log --nce 24 ,28,30,32,34,38,42 --out evaluation.csv --assure_notrain 1 Predict Spectra You can use it from python like so ... ... Note: The main branch right now has a problem where spectrum plotting (exporting to spectrum_utils is broken) import sys import torch import elfragmentador as ef from elfragmentador.model import PepTransformerModel from loguru import logger # This makes a lot slimmer the logging ingofrmation logger . remove () logger . add ( sys . stderr , level = \"WARNING\" ) checkpoint_path = \"some/path/to/a/checkpoint\" # or checkpoint_path = ef . DEFAULT_CHECKPOINT model = PepTransformerModel . load_from_checkpoint ( checkpoint_path ) # Set the model as evaluation mode _ = model . eval () with torch . no_grad (): tensor_predictions = model . predict_from_seq ( \"MYPEPTIDEK/2\" , nce = 27.0 ) # PredictionResults(irt=tensor([0.2022], grad_fn=<SqueezeBackward1>), spectra=tensor([0.0000e+00, ...grad_fn=<SqueezeBackward1>)) # or ... import matplotlib.pyplot as plt spectrum_prediction = model . predict_from_seq ( \"MYPEPTIDEK/3\" , nce = 27.0 , as_spectrum = True ) spectrum_prediction = model . predict_from_seq ( \"AAESLQRAEATNAELER/2\" , nce = 22.0 , as_spectrum = True ) spectrum_prediction . plot () plt . show () stderr Downloading: \"https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt\" to /home/runner/.cache/torch/hub/checkpoints/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt Display 0%| | 0.00/3.54M [00:00<?, ?B/s] stderr Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v1.9.3. To apply the upgrade to your files permanently, run python -m pytorch_lightning.utilities.upgrade_checkpoint --file https:/github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt stderr /home/runner/.cache/pypoetry/virtualenvs/elfragmentador-ZM4HrtcN-py3.10/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called full_state_update that has not been set for this class (MissingDataAverager). The property determines if update by default needs access to the full metric state. If this is not the case, significant speedups can be achieved and we recommend setting this to False . We provide an checking function from torchmetrics.utilities import check_forward_full_state_property that can be used to check if the full_state_update=True (old and potential slower behaviour, default for now) or if full_state_update=False can be used safely. warnings.warn( args, *kwargs) Display Training Training is handled by calling a training script from the shell... this would be an example. # Be a good person and keep track of your experiments, use wandb $ wandb login elfragmentador_train \\ --run_name onecycle_5e_petite_ndl4 \\ --scheduler onecycle \\ --max_epochs 5 \\ --lr_ratio 25 \\ --terminator_patience 20 \\ --lr 0 .00005 \\ --gradient_clip_val 1 .0 \\ --dropout 0 .1 \\ --nhead 4 \\ --nhid 512 \\ --ninp 224 \\ --num_decoder_layers 4 \\ --num_encoder_layers 2 \\ --batch_size 400 \\ --accumulate_grad_batches 1 \\ --precision 16 \\ --gpus 1 \\ --progress_bar_refresh_rate 5 \\ --data_dir /content/20210217-traindata","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"For a more detailed overview of how to use the command line, check the CLI Reference section of the documentation If you woul like any help/would like something implemented, feel free to open a github issue or message me :)","title":"Quickstart"},{"location":"quickstart/#installation","text":"This is currently being distributed as a pypi package, to get the latest version use the following ... User Install: pip install elfragmentador Development install: git clone https://github.com/jspaezp/elfragmentador.git cd elfragmentador pip install /content/elfragmentador # or ... poetry install","title":"Installation"},{"location":"quickstart/#usage","text":"","title":"Usage"},{"location":"quickstart/#prediction","text":"$ elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 7 --out foo.dlib","title":"Prediction"},{"location":"quickstart/#rescoring","text":"This will add a couple of columns to a percolator input file, which can be used before you run it though percolator or mokapot! elfragmentador append_pin --pin { input .pin } --out { output .pin } --nce 30 .0","title":"Rescoring"},{"location":"quickstart/#check-performance","text":"I have implemented a way to compare the predictions of the model with an several spectral librariy-related files. The easiest one for is the mokapot .peptides.txt file after a comet search. $ poetry run elfragmentador evaluate --input mokapot.peptides.txt.evaluation.log --nce 24 ,28,30,32,34,38,42 --out evaluation.csv --assure_notrain 1","title":"Check performance"},{"location":"quickstart/#predict-spectra","text":"You can use it from python like so ... ... Note: The main branch right now has a problem where spectrum plotting (exporting to spectrum_utils is broken) import sys import torch import elfragmentador as ef from elfragmentador.model import PepTransformerModel from loguru import logger # This makes a lot slimmer the logging ingofrmation logger . remove () logger . add ( sys . stderr , level = \"WARNING\" ) checkpoint_path = \"some/path/to/a/checkpoint\" # or checkpoint_path = ef . DEFAULT_CHECKPOINT model = PepTransformerModel . load_from_checkpoint ( checkpoint_path ) # Set the model as evaluation mode _ = model . eval () with torch . no_grad (): tensor_predictions = model . predict_from_seq ( \"MYPEPTIDEK/2\" , nce = 27.0 ) # PredictionResults(irt=tensor([0.2022], grad_fn=<SqueezeBackward1>), spectra=tensor([0.0000e+00, ...grad_fn=<SqueezeBackward1>)) # or ... import matplotlib.pyplot as plt spectrum_prediction = model . predict_from_seq ( \"MYPEPTIDEK/3\" , nce = 27.0 , as_spectrum = True ) spectrum_prediction = model . predict_from_seq ( \"AAESLQRAEATNAELER/2\" , nce = 22.0 , as_spectrum = True ) spectrum_prediction . plot () plt . show () stderr Downloading: \"https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt\" to /home/runner/.cache/torch/hub/checkpoints/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt Display 0%| | 0.00/3.54M [00:00<?, ?B/s] stderr Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v1.9.3. To apply the upgrade to your files permanently, run python -m pytorch_lightning.utilities.upgrade_checkpoint --file https:/github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt stderr /home/runner/.cache/pypoetry/virtualenvs/elfragmentador-ZM4HrtcN-py3.10/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called full_state_update that has not been set for this class (MissingDataAverager). The property determines if update by default needs access to the full metric state. If this is not the case, significant speedups can be achieved and we recommend setting this to False . We provide an checking function from torchmetrics.utilities import check_forward_full_state_property that can be used to check if the full_state_update=True (old and potential slower behaviour, default for now) or if full_state_update=False can be used safely. warnings.warn( args, *kwargs) Display","title":"Predict Spectra"},{"location":"quickstart/#training","text":"Training is handled by calling a training script from the shell... this would be an example. # Be a good person and keep track of your experiments, use wandb $ wandb login elfragmentador_train \\ --run_name onecycle_5e_petite_ndl4 \\ --scheduler onecycle \\ --max_epochs 5 \\ --lr_ratio 25 \\ --terminator_patience 20 \\ --lr 0 .00005 \\ --gradient_clip_val 1 .0 \\ --dropout 0 .1 \\ --nhead 4 \\ --nhid 512 \\ --ninp 224 \\ --num_decoder_layers 4 \\ --num_encoder_layers 2 \\ --batch_size 400 \\ --accumulate_grad_batches 1 \\ --precision 16 \\ --gpus 1 \\ --progress_bar_refresh_rate 5 \\ --data_dir /content/20210217-traindata","title":"Training"},{"location":"API%20reference/PepTransformerModel/","text":"PepTransformerModel elfragmentador.model Attributes elfragmentador . model . LiteralFalse = Literal [ False ] module-attribute Classes elfragmentador . model . PepTransformerModel ( num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder : str = 'linear' , lr : float = 0.0001 , scheduler : str = 'plateau' , lr_ratio : float | int = 200 , steps_per_epoch : None = None , loss_ratio : float = 5 , args , kwargs ) -> None Bases: pl . LightningModule PepTransformerModel Predicts retention times and HCD spectra from peptides. init Instantiates the class. Generates a new instance of the PepTransformerModel PARAMETER DESCRIPTION num_decoder_layers int, optional Number of layers in the transformer decoder, by default 6 DEFAULT: 6 num_encoder_layers int, optional Number of laters in the transformer encoder, by default 6 DEFAULT: 6 nhid int, optional Number of dimensions used in the feedforward networks inside the transformer encoder and decoders, by default 2024 DEFAULT: 2024 d_model int, optional Number of features to pass to the transformer encoder. The embedding transforms the input to this input, by default 516 DEFAULT: 516 nhead int, optional Number of multi-attention heads in the transformer, by default 4 DEFAULT: 4 dropout float, optional dropout, by default 0.1 DEFAULT: 0.1 combine_embeds bool, optional Whether the embeddings for modifications and sequences should be shared for irt and fragment predictions TYPE: bool DEFAULT: True combine_encoders bool = True, Whether the transformer encoders for for irt and fragments should be shared. TYPE: bool DEFAULT: True lr float, optional Learning rate, by default 1e-4 DEFAULT: 0.0001 scheduler str, optional What scheduler to use, check the available ones with PepTransformerModel.accepted_schedulers , by default \"plateau\" DEFAULT: 'plateau' lr_ratio Union[float, int], optional For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1. For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn rate would be 1.0. by default 200 DEFAULT: 200 steps_per_epoch None, optional expected number of steps per epoch, used internally to calculate learning rates when using the oncecycle scheduler, by default None DEFAULT: None loss_ratio float, optional The ratio of the spectrum to retention time loss to use when adding before passing to the optimizer. Higher values mean more weight to spectra with respect to the retention time. By default 5 TYPE: float DEFAULT: 5 Source code in elfragmentador/model/__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder : str = \"linear\" , lr : float = 1e-4 , scheduler : str = \"plateau\" , lr_ratio : float | int = 200 , steps_per_epoch : None = None , loss_ratio : float = 5 , * args , ** kwargs , ) -> None : \"\"\" __init__ Instantiates the class. Generates a new instance of the PepTransformerModel Parameters: num_decoder_layers : int, optional Number of layers in the transformer decoder, by default 6 num_encoder_layers : int, optional Number of laters in the transformer encoder, by default 6 nhid : int, optional Number of dimensions used in the feedforward networks inside the transformer encoder and decoders, by default 2024 d_model : int, optional Number of features to pass to the transformer encoder. The embedding transforms the input to this input, by default 516 nhead : int, optional Number of multi-attention heads in the transformer, by default 4 dropout : float, optional dropout, by default 0.1 combine_embeds: bool, optional Whether the embeddings for modifications and sequences should be shared for irt and fragment predictions combine_encoders: bool = True, Whether the transformer encoders for for irt and fragments should be shared. lr : float, optional Learning rate, by default 1e-4 scheduler : str, optional What scheduler to use, check the available ones with `PepTransformerModel.accepted_schedulers`, by default \"plateau\" lr_ratio : Union[float, int], optional For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1. For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn rate would be 1.0. by default 200 steps_per_epoch : None, optional expected number of steps per epoch, used internally to calculate learning rates when using the oncecycle scheduler, by default None loss_ratio: float, optional The ratio of the spectrum to retention time loss to use when adding before passing to the optimizer. Higher values mean more weight to spectra with respect to the retention time. By default 5 \"\"\" super () . __init__ () self . ms2ml_config = CONFIG self . NUM_FRAGMENT_EMBEDDINGS = self . ms2ml_config . num_fragment_embeddings self . save_hyperparameters () logger . info ( f \"num_decoder_layers { num_decoder_layers } \" f \"num_encoder_layers { num_encoder_layers } \" f \"nhid { nhid } d_model { d_model } \" f \"nhead { nhead } dropout { dropout } \" f \"combined embeds { combine_embeds } combined encoders { combine_encoders } \" ) self . main_model = PepTransformerBase ( num_fragments = self . NUM_FRAGMENT_EMBEDDINGS , num_decoder_layers = num_decoder_layers , num_encoder_layers = num_encoder_layers , nhid = nhid , d_model = d_model , nhead = nhead , dropout = dropout , combine_embeds = combine_embeds , combine_encoders = combine_encoders , final_decoder = final_decoder , ) self . metric_calculator = MetricCalculator () self . mse_loss = nn . MSELoss ( reduction = \"none\" ) self . cosine_loss = CosineLoss ( dim = 1 , eps = 1e-8 ) self . angle_loss = SpectralAngleLoss ( dim = 1 , eps = 1e-8 ) # Training related things self . lr = lr assert ( scheduler in self . accepted_schedulers ), f \"Passed scheduler ' { scheduler } is not one of { self . accepted_schedulers } \" self . scheduler = scheduler self . lr_ratio = lr_ratio self . steps_per_epoch = steps_per_epoch self . loss_ratio = loss_ratio self . irt_metric = MissingDataAverager () self . loss_metric = MissingDataAverager () self . spectra_metric = MissingDataAverager () self . spectra_metric2 = MissingDataAverager () Attributes accepted_schedulers = [ 'plateau' , 'cosine' , 'onecycle' ] class-attribute __version__ = elfragmentador . __version__ class-attribute ms2ml_config = CONFIG instance-attribute NUM_FRAGMENT_EMBEDDINGS = self . ms2ml_config . num_fragment_embeddings instance-attribute main_model = PepTransformerBase ( num_fragments = self . NUM_FRAGMENT_EMBEDDINGS , num_decoder_layers = num_decoder_layers , num_encoder_layers = num_encoder_layers , nhid = nhid , d_model = d_model , nhead = nhead , dropout = dropout , combine_embeds = combine_embeds , combine_encoders = combine_encoders , final_decoder = final_decoder ) instance-attribute metric_calculator = MetricCalculator () instance-attribute mse_loss = nn . MSELoss ( reduction = 'none' ) instance-attribute cosine_loss = CosineLoss ( dim = 1 , eps = 1e-08 ) instance-attribute angle_loss = SpectralAngleLoss ( dim = 1 , eps = 1e-08 ) instance-attribute lr = lr instance-attribute scheduler = scheduler instance-attribute lr_ratio = lr_ratio instance-attribute steps_per_epoch = steps_per_epoch instance-attribute loss_ratio = loss_ratio instance-attribute irt_metric = MissingDataAverager () instance-attribute loss_metric = MissingDataAverager () instance-attribute spectra_metric = MissingDataAverager () instance-attribute spectra_metric2 = MissingDataAverager () instance-attribute Functions summarize ( max_depth = 3 ) Source code in elfragmentador/model/__init__.py 167 168 def summarize ( self , max_depth = 3 ): return summarize ( self , max_depth ) forward ( seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor ) Source code in elfragmentador/model/__init__.py 170 171 172 173 174 175 176 177 def forward ( self , seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor , ): return self . main_model . forward ( seq = seq , mods = mods , charge = charge , nce = nce ) predict_from_seq ( seq : str , nce : float , as_spectrum = False ) -> PredictionResults | Spectrum Source code in elfragmentador/model/__init__.py 179 180 181 182 183 184 185 186 187 188 189 def predict_from_seq ( self , seq : str , nce : float , as_spectrum = False , ) -> PredictionResults | Spectrum : return self . main_model . predict_from_seq ( seq = seq , nce = nce , as_spectrum = as_spectrum , ) torch_batch_from_seq ( args , kwargs ) -> ForwardBatch staticmethod Source code in elfragmentador/model/__init__.py 191 192 193 194 @staticmethod def torch_batch_from_seq ( * args , ** kwargs ) -> ForwardBatch : torch_batch_from_seq . __doc__ return torch_batch_from_seq ( * args , ** kwargs ) to_torchscript () Convert the model to torchscript. Example: model = PepTransformerModel() ts = model.to_torchscript() type(ts) Source code in elfragmentador/model/__init__.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def to_torchscript ( self ): \"\"\" Convert the model to torchscript. Example: >>> model = PepTransformerModel() >>> ts = model.to_torchscript() >>> type(ts) <class 'torch.jit._trace.TopLevelTracedModule'> \"\"\" _fake_input_data_torchscript = self . torch_batch_from_seq ( seq = \"MYM[U:35]DIFIEDPEPTYDE\" , charge = 3 , nce = 27.0 ) backup_calculator = self . metric_calculator self . metric_calculator = None bkp_1 = self . main_model . decoder . nce_encoder . static_size self . main_model . decoder . nce_encoder . static_size = self . NUM_FRAGMENT_EMBEDDINGS bkp_2 = self . main_model . decoder . charge_encoder . static_size self . main_model . decoder . charge_encoder . static_size = ( self . NUM_FRAGMENT_EMBEDDINGS ) script = super () . to_torchscript ( example_inputs = _fake_input_data_torchscript , method = \"trace\" ) self . main_model . decoder . nce_encoder . static_size = bkp_1 self . main_model . decoder . charge_encoder . static_size = bkp_2 self . main_model . metric_calculator = backup_calculator return script add_model_specific_args ( parser : _ArgumentGroup ) -> _ArgumentGroup staticmethod Add_model_specific_args Adds arguments to a parser. It is used to add the command line arguments for the training/generation of the model. PARAMETER DESCRIPTION parser An argparser parser (anything that has the .add_argument method) to which the arguments will be added TYPE: _ArgumentGroup RETURNS DESCRIPTION _ArgumentGroup _ArgumentGroup, the same parser with the added arguments Source code in elfragmentador/model/__init__.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @staticmethod def add_model_specific_args ( parser : _ArgumentGroup ) -> _ArgumentGroup : \"\"\" Add_model_specific_args Adds arguments to a parser. It is used to add the command line arguments for the training/generation of the model. Args: parser (_ArgumentGroup): An argparser parser (anything that has the `.add_argument` method) to which the arguments will be added Returns: _ArgumentGroup, the same parser with the added arguments \"\"\" parser . add_argument ( \"--num_queries\" , default = 150 , type = int , help = \"Expected encoding length of the spectra\" , ) parser . add_argument ( \"--num_decoder_layers\" , default = 6 , type = int , help = \"Number of sub-encoder-layers in the encoder\" , ) parser . add_argument ( \"--num_encoder_layers\" , default = 6 , type = int , help = \"Number of sub-encoder-layers in the decoder\" , ) parser . add_argument ( \"--nhid\" , default = 1024 , type = int , help = \"Dimension of the feedforward networks\" , ) parser . add_argument ( \"--d_model\" , default = 516 , type = int , help = \"Number of input features to the transformer encoder\" , ) parser . add_argument ( \"--nhead\" , default = 12 , type = int , help = \"Number of attention heads\" ) parser . add_argument ( \"--dropout\" , default = 0.1 , type = float ) parser . add_argument ( \"--combine_embeds\" , action = argparse . BooleanOptionalAction , help = ( \"Whether the embeddings for aminoacid and modifications\" \" should be shared between the irt and fragment sections\" ), ) parser . add_argument ( \"--combine_encoders\" , action = argparse . BooleanOptionalAction , help = ( \"Whether the encoders for aminoacid and modifications\" \" should be shared between the irt and fragment sections\" ), ) parser . add_argument ( \"--final_decoder\" , default = \"mlp\" , type = str , help = ( \"What kind of final layer should the docer have to\" \" output a single number, options are 'mlp' and 'linear'\" ), ) parser . add_argument ( \"--lr\" , default = 1e-4 , type = float ) parser . add_argument ( \"--scheduler\" , default = \"plateau\" , type = str , help = ( \"Scheduler to use during training, \" f \"either of { PepTransformerModel . accepted_schedulers } \" ), ) parser . add_argument ( \"--lr_ratio\" , default = 200.0 , type = float , help = ( \"For cosine annealing: \" \"Ratio of the initial learning rate to use with cosine annealing\" \" for instance a lr or 1 and a ratio of 10 would have a minimum\" \" learning rate of 0.1 \\n \" \"For onecycle: \" \"Ratio of the initial lr and and maximum one, \" \"for instance if lr is 0.1 and ratio is 10, the max learn rate\" \"would be 1.0\" ), ) parser . add_argument ( \"--loss_ratio\" , default = 5.0 , type = float , help = ( \"Ratio between the retention time and the spectrum loss\" \" (higher values mean more weight to the spectra loss\" \" with respect to the retention time loss)\" ), ) return parser configure_scheduler_plateau ( optimizer , lr_ratio ) staticmethod Source code in elfragmentador/model/__init__.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @staticmethod def configure_scheduler_plateau ( optimizer , lr_ratio ): assert lr_ratio < 1 scheduler_dict = { \"scheduler\" : torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer = optimizer , mode = \"min\" , factor = lr_ratio , patience = 2 , verbose = False , ), \"interval\" : \"epoch\" , \"monitor\" : \"val_l\" , } return scheduler_dict configure_scheduler_cosine ( optimizer , lr_ratio , min_lr ) staticmethod Source code in elfragmentador/model/__init__.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 @staticmethod def configure_scheduler_cosine ( optimizer , lr_ratio , min_lr ): assert lr_ratio > 1 scheduler_dict = { \"scheduler\" : CosineAnnealingWarmRestarts ( optimizer = optimizer , T_0 = 1 , T_mult = 2 , eta_min = min_lr , last_epoch =- 1 , verbose = False , ), \"interval\" : \"step\" , } return scheduler_dict configure_scheduler_oncecycle ( optimizer , lr_ratio , learning_rate , steps_per_epoch , accumulate_grad_batches , max_epochs ) staticmethod Source code in elfragmentador/model/__init__.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 @staticmethod def configure_scheduler_oncecycle ( optimizer , lr_ratio , learning_rate , steps_per_epoch , accumulate_grad_batches , max_epochs , ): max_lr = learning_rate * lr_ratio spe = steps_per_epoch // accumulate_grad_batches pct_start = 0.3 logger . info ( f \">> Scheduler setup: max_lr { max_lr } , \" f \"Max Epochs: { max_epochs } , \" f \"Steps per epoch: { steps_per_epoch } , \" f \"SPE (after accum grad batches) { spe } , \" f \"Percent Warmup { pct_start } , \" f \"Accumulate Batches { accumulate_grad_batches } , \" ) scheduler_dict = { \"scheduler\" : torch . optim . lr_scheduler . OneCycleLR ( optimizer = optimizer , max_lr = max_lr , epochs = max_epochs , pct_start = pct_start , steps_per_epoch = spe , ), \"interval\" : \"step\" , } return scheduler_dict configure_optimizers () -> tuple [ list [ AdamW ], list [ dict [ str , ReduceLROnPlateau | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , CosineAnnealingWarmRestarts | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , OneCycleLR | str ]]] Configure_optimizers COnfigures the optimizers for training. It is internally used by pytorch_lightning during training, so far I implemented 3 options (set when making the module). OneCycleLR seems to give the best results overall in the least amount of time. The only tradeoff that I see is that resuming training does not seem to be really easy. Check the pytorch_lightning documentation to see how this is used in the training loop RETURNS DESCRIPTION tuple [ list [ AdamW ], list [ dict [ str , ReduceLROnPlateau | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , CosineAnnealingWarmRestarts | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , OneCycleLR | str ]]] Two lists, one containing the optimizer and another contining the scheduler. Source code in elfragmentador/model/__init__.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 def configure_optimizers ( self , ) -> ( tuple [ list [ AdamW ], list [ dict [ str , ReduceLROnPlateau | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , CosineAnnealingWarmRestarts | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , OneCycleLR | str ]]] ): \"\"\" Configure_optimizers COnfigures the optimizers for training. It is internally used by pytorch_lightning during training, so far I implemented 3 options (set when making the module). OneCycleLR seems to give the best results overall in the least amount of time. The only tradeoff that I see is that resuming training does not seem to be really easy. Check the pytorch_lightning documentation to see how this is used in the training loop Returns: Two lists, one containing the optimizer and another contining the scheduler. \"\"\" opt = torch . optim . AdamW ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . lr , betas = ( 0.9 , 0.98 ), ) if self . scheduler == \"plateau\" : sched_dict = self . configure_scheduler_plateau ( optimizer = opt , lr_ratio = self . lr_ratio ) elif self . scheduler == \"cosine\" : sched_dict = self . configure_scheduler_cosine ( optimizer = opt , lr_ratio = self . lr_ratio , min_lr = self . lr / self . lr_ratio ) elif self . scheduler == \"onecycle\" : assert self . steps_per_epoch is not None , \"Please set steps_per_epoch\" if self . trainer . max_epochs == 1000 : warnings . warn ( \"Max epochs was 1000, make sure you want this\" ) if self . lr_ratio > 20 : warnings . warn ( f \"Provided LR ratio ' { self . lr_ratio } ' seems a lil high,\" \" make sure you want that for the OneCycleLR scheduler\" ) time . sleep ( 3 ) # just so the user has time to see the message... sched_dict = self . configure_scheduler_oncecycle ( optimizer = opt , lr_ratio = self . lr_ratio , learning_rate = self . lr , steps_per_epoch = self . steps_per_epoch , accumulate_grad_batches = self . trainer . accumulate_grad_batches , max_epochs = self . trainer . max_epochs , ) else : raise ValueError ( \"Scheduler should be one of 'plateau' or 'cosine', passed: \" , self . scheduler , ) # TODO check if using different optimizers for different parts of the # model would work better logger . info ( f \" \\n\\n >>> Setting up schedulers: \\n\\n { sched_dict } \" ) return [ opt ], [ sched_dict ] plot_scheduler_lr () Plot the learning rate of the scheduler. This is useful to see how the learning rate changes during training, and to make sure that the scheduler is working as intended. Source code in elfragmentador/model/__init__.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 def plot_scheduler_lr ( self ): \"\"\" Plot the learning rate of the scheduler. This is useful to see how the learning rate changes during training, and to make sure that the scheduler is working as intended. \"\"\" steps_per_epoch = self . steps_per_epoch if steps_per_epoch is None : steps_per_epoch = 1000 try : accumulate_grad_batches = self . trainer . accumulate_grad_batches max_epochs = self . trainer . max_epochs except RuntimeError : accumulate_grad_batches = 1 max_epochs = 10 spe = steps_per_epoch // accumulate_grad_batches optimizer , schedulers = self . configure_optimizers () optimizer = optimizer [ 0 ] scheduler = schedulers [ 0 ][ \"scheduler\" ] xs = list ( range ( spe * max_epochs )) lrs = [] for i in xs : optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) scheduler . step () str_list = uniplot . plot_to_string ( np . log1p ( np . array ( lrs )), xs , title = \"Learning Rate Schedule\" ) plot_str = \" \\n \" . join ( str_list ) logger . info ( f \" \\n\\n { plot_str } \\n\\n \" ) training_step ( batch : TrainBatch , batch_idx : int | None = None ) -> Tensor See pytorch_lightning documentation. Source code in elfragmentador/model/__init__.py 600 601 602 603 604 605 606 607 608 609 610 611 612 def training_step ( self , batch : TrainBatch , batch_idx : int | None = None ) -> Tensor : \"\"\"See pytorch_lightning documentation.\"\"\" step_out = self . _step ( batch , batch_idx = batch_idx ) log_dict = { \"train_\" + k : v for k , v in step_out . items ()} log_dict . update ({ \"LR\" : self . trainer . optimizers [ 0 ] . param_groups [ 0 ][ \"lr\" ]}) self . log_dict ( log_dict , prog_bar = True , # reduce_fx=nanmean, ) return step_out [ \"l\" ] on_train_start () -> None Source code in elfragmentador/model/__init__.py 614 615 616 617 def on_train_start ( self ) -> None : logger . info ( \"Weights before the start of the training epoch:\" ) logger . info ( copy . deepcopy ( self . state_dict ())) return super () . on_train_start () validation_step ( batch : TrainBatch , batch_idx : int | None = None ) -> Tensor See pytorch_lightning documentation. Source code in elfragmentador/model/__init__.py 619 620 621 622 623 624 625 626 627 628 629 630 def validation_step ( self , batch : TrainBatch , batch_idx : int | None = None ) -> Tensor : \"\"\"See pytorch_lightning documentation.\"\"\" step_out = self . _step ( batch , batch_idx = batch_idx ) self . irt_metric . update ( step_out [ \"irt_l\" ]) self . loss_metric . update ( step_out [ \"l\" ]) self . spectra_metric . update ( step_out [ \"spec_l\" ]) self . spectra_metric2 . update ( step_out [ \"spec_l2\" ]) return step_out [ \"l\" ] validation_epoch_end ( outputs : list [ Tensor ]) -> list [ Tensor ] See pytorch lightning documentation. Source code in elfragmentador/model/__init__.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def validation_epoch_end ( self , outputs : list [ Tensor ]) -> list [ Tensor ]: \"\"\"See pytorch lightning documentation.\"\"\" log_dict = { \"val_irt_l\" : self . irt_metric . compute (), \"val_l\" : self . loss_metric . compute (), \"val_spec_l\" : self . spectra_metric . compute (), \"val_spec_l2\" : self . spectra_metric2 . compute (), } self . log_dict ( log_dict , prog_bar = True , ) self . irt_metric . reset () self . loss_metric . reset () self . spectra_metric . reset () self . spectra_metric2 . reset () return super () . validation_epoch_end ( outputs ) test_step ( batch , batch_idx : int | None = None ) -> tuple [ dict [ str , Tensor ], PredictionResults ] Source code in elfragmentador/model/__init__.py 686 687 688 689 690 def test_step ( self , batch , batch_idx : int | None = None ) -> tuple [ dict [ str , Tensor ], PredictionResults ]: losses , pred_out = self . _evaluation_step ( batch = batch , batch_idx = batch_idx ) return losses , pred_out . irt , batch . irt test_epoch_end ( results : list ) Source code in elfragmentador/model/__init__.py 692 693 694 695 def test_epoch_end ( self , results : list ): self . metric_calculator . trainer = self . trainer self . metric_calculator . log_dict = self . log_dict return self . metric_calculator . test_epoch_end ( results ) predict_step ( batch : TrainBatch , batch_idx : int | None = None ) Source code in elfragmentador/model/__init__.py 697 698 699 700 701 702 def predict_step ( self , batch : TrainBatch , batch_idx : int | None = None ): yhat_irt , yhat_spectra = self . forward ( seq = batch . seq , mods = batch . mods , charge = batch . charge , nce = batch . nce ) pred_out = PredictionResults ( irt = yhat_irt , spectra = torch . relu ( yhat_spectra )) return pred_out on_after_backward () Source code in elfragmentador/model/__init__.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 def on_after_backward ( self ): msg = [] global_step = self . global_step if ( global_step % 50 ) == 0 : for name , param in self . named_parameters (): if \"weight\" in name and \"norm\" not in name : if param . requires_grad : try : if param . grad is None : raise AttributeError if any ( x in name for x in [ \"aa_encoder.weight\" , \"mod_encoder.weight\" , \"trans_decoder_embedding.weight\" , ] ): val = param . grad . abs () . mean () if torch . any ( torch . isnan ( val )): logger . error ( f \"nan mean gradient for { name } : { param . grad } \" ) self . log ( name , val , prog_bar = True , on_step = True ) except AttributeError : msg . append ( name ) except ValueError : msg . append ( name ) if len ( msg ) > 0 : logger . warning ( \" \" . join ( msg ) + \"Did not have gradients in step {global_step} \" ) on_train_epoch_end () -> None Source code in elfragmentador/model/__init__.py 738 739 740 def on_train_epoch_end ( self ) -> None : evaluate_landmark_rt ( self ) return super () . on_train_epoch_end () load_from_checkpoint ( args , kwargs ) classmethod Source code in elfragmentador/model/__init__.py 742 743 744 745 746 @classmethod def load_from_checkpoint ( cls , * args , ** kwargs ): mod = super () . load_from_checkpoint ( * args , ** kwargs ) evaluate_landmark_rt ( mod ) return mod Functions elfragmentador . model . evaluate_landmark_rt ( model : PepTransformerModel ) Checks the prediction of the model on the iRT peptides. Predicts all the procal and Biognosys iRT peptides and checks the correlation of the theoretical iRT values and the predicted ones PARAMETER DESCRIPTION model PepTransformerModel A model to test the predictions on TYPE: PepTransformerModel Source code in elfragmentador/model/__init__.py 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def evaluate_landmark_rt ( model : PepTransformerModel ): \"\"\"Checks the prediction of the model on the iRT peptides. Predicts all the procal and Biognosys iRT peptides and checks the correlation of the theoretical iRT values and the predicted ones Parameters: model: PepTransformerModel A model to test the predictions on \"\"\" model . eval () real_rt = [] pred_rt = [] for seq , desc in IRT_PEPTIDES . items (): with torch . no_grad (): out = model . predict_from_seq ( f \" { seq } /2\" , 25 ) pred_rt . append ( out . irt . clone () . cpu () . numpy ()) real_rt . append ( np . array ( desc [ \"irt\" ])) fit = polyfit ( np . array ( real_rt ) . flatten (), np . array ( pred_rt ) . flatten ()) logger . info ( fit ) plot_str = uniplot . plot_to_string ( xs = np . array ( real_rt ) . flatten (), ys = np . array ( pred_rt ) . flatten (), title = \"Prediction vs real iRT of biognosys and procal peptides\" , ) logger . info ( \" \\n \" + \" \\n \" . join ( plot_str ), \" \\n \" ) return fit , plot_str elfragmentador.model.peptransformer Classes elfragmentador . model . peptransformer . PepTransformerBase ( num_fragments , num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder = 'linear' ) -> None Bases: nn . Module Source code in elfragmentador/model/peptransformer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , num_fragments , num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder = \"linear\" , ) -> None : super () . __init__ () # Peptide encoder self . encoder = PeptideTransformerEncoder ( d_model = d_model , dropout = dropout , nhead = nhead , nhid = nhid , layers = num_encoder_layers , ) # Peptide decoder self . decoder = FragmentTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = nhid , layers = num_decoder_layers , dropout = dropout , num_fragments = num_fragments , final_decoder = final_decoder , ) self . irt_decoder = IRTDecoder ( d_model = d_model , dim_feedforward = nhid , nhead = nhead , n_layers = num_encoder_layers , dropout = dropout , final_decoder = final_decoder , ) if combine_embeds : self . irt_decoder . aa_embed = self . encoder . aa_embed if combine_encoders : self . irt_decoder . encoder = self . encoder . encoder Attributes encoder = PeptideTransformerEncoder ( d_model = d_model , dropout = dropout , nhead = nhead , nhid = nhid , layers = num_encoder_layers ) instance-attribute decoder = FragmentTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = nhid , layers = num_decoder_layers , dropout = dropout , num_fragments = num_fragments , final_decoder = final_decoder ) instance-attribute irt_decoder = IRTDecoder ( d_model = d_model , dim_feedforward = nhid , nhead = nhead , n_layers = num_encoder_layers , dropout = dropout , final_decoder = final_decoder ) instance-attribute Functions forward ( seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor ) -> PredictionResults Forward Generate predictions. Privides the function for the forward pass to the model. PARAMETER DESCRIPTION seq Encoded pepide sequence [B, L] (view details) TYPE: Tensor mods Encoded modification sequence [B, L], by default None TYPE: Tensor nce float Tensor with the charges [B, 1] TYPE: Tensor charge long Tensor with the charges [B, 1], by default None TYPE: Tensor Details seq: The peptide is encoded as integers for the aminoacid. \"AAA\" encoded for a max length of 5 would be torch.Tensor([ 1, 1, 1, 0, 0]).long() nce: Normalized collision energy to use during the prediction. charge: A tensor corresponding to the charges of each of the peptide precursors (long) mods: Modifications encoded as integers Source code in elfragmentador/model/peptransformer.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def forward ( self , seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor , ) -> PredictionResults : \"\"\" Forward Generate predictions. Privides the function for the forward pass to the model. Parameters: seq (Tensor): Encoded pepide sequence [B, L] (view details) mods (Tensor): Encoded modification sequence [B, L], by default None nce (Tensor): float Tensor with the charges [B, 1] charge (Tensor): long Tensor with the charges [B, 1], by default None Details: seq: The peptide is encoded as integers for the aminoacid. \"AAA\" encoded for a max length of 5 would be torch.Tensor([ 1, 1, 1, 0, 0]).long() nce: Normalized collision energy to use during the prediction. charge: A tensor corresponding to the charges of each of the peptide precursors (long) mods: Modifications encoded as integers \"\"\" trans_encoder_output , mem_mask = self . encoder ( seq = seq , mods = mods ) rt_output = self . irt_decoder ( seq = seq , mods = mods ) spectra_output = self . decoder ( memory = trans_encoder_output , charge = charge , nce = nce , memory_key_padding_mask = mem_mask , ) return PredictionResults ( irt = rt_output , spectra = spectra_output ) predict_from_seq ( seq : str , nce : float , as_spectrum = False ) -> PredictionResults | AnnotatedPeptideSpectrum Predict_from_seq Predicts spectra from a sequence as a string. Utility method that gets a sequence as a string, encodes it internally to the correct input form and outputs the predicted spectra. Note that the spectra is not decoded as an output, please check elfragmentador.encoding_decoding.decode_fragment_tensor for the decoding. The irt is scaled by 100 and is in the Biognosys scale. TODO: consider if the output should be decoded ... PARAMETER DESCRIPTION seq Sequence to use for prediction, supports modifications in the form of S[PHOSPHO], S[+80] and T[181] TYPE: str nce Normalized collision energy to use during the prediction TYPE: float as_spectrum Wether to return a Spectrum object instead of the raw tensor predictions (Default value = False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION PredictionResults A named tuple with two named results; irt and spectra TYPE: PredictionResults | AnnotatedPeptideSpectrum Spectrum A spectrum object with the predicted spectrum TYPE: PredictionResults | AnnotatedPeptideSpectrum Examples: >>> import pytorch_lightning as pl >>> from elfragmentador.config import CONFIG >>> pl . seed_everything ( 42 ) 42 >>> my_model = PepTransformerBase ( num_fragments = CONFIG . num_fragment_embeddings ) # Or load the model from a checkpoint >>> _ = my_model . eval () >>> my_model . predict_from_seq ( \"MYPEPT[U:21]IDEK/3\" , 27 ) PredictionResults(irt=tensor(...), spectra=tensor([...])) >>> out = my_model . predict_from_seq ( \"MYPEPT[U:21]IDEK/3\" , 27 , as_spectrum = True ) >>> type ( out ) <class 'ms2ml.spectrum.AnnotatedPeptideSpectrum'> >>> # my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) Source code in elfragmentador/model/peptransformer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @torch . no_grad () def predict_from_seq ( self , seq : str , nce : float , as_spectrum = False , ) -> PredictionResults | AnnotatedPeptideSpectrum : \"\"\" Predict_from_seq Predicts spectra from a sequence as a string. Utility method that gets a sequence as a string, encodes it internally to the correct input form and outputs the predicted spectra. Note that the spectra is not decoded as an output, please check `elfragmentador.encoding_decoding.decode_fragment_tensor` for the decoding. The irt is scaled by 100 and is in the Biognosys scale. TODO: consider if the output should be decoded ... Parameters: seq (str): Sequence to use for prediction, supports modifications in the form of S[PHOSPHO], S[+80] and T[181] nce (float): Normalized collision energy to use during the prediction as_spectrum (bool, optional): Wether to return a Spectrum object instead of the raw tensor predictions (Default value = False) Returns: PredictionResults: A named tuple with two named results; irt and spectra Spectrum: A spectrum object with the predicted spectrum Examples: >>> import pytorch_lightning as pl >>> from elfragmentador.config import CONFIG >>> pl.seed_everything(42) 42 >>> my_model = PepTransformerBase(num_fragments=CONFIG.num_fragment_embeddings) # Or load the model from a checkpoint >>> _ = my_model.eval() >>> my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) PredictionResults(irt=tensor(...), spectra=tensor([...])) >>> out = my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27, \\ as_spectrum=True) >>> type(out) <class 'ms2ml.spectrum.AnnotatedPeptideSpectrum'> >>> # my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) \"\"\" # noqa in_batch = Tensorizer () . convert_string ( data = seq , nce = nce ) device = next ( self . parameters ()) . device in_batch_dict = { k : v . clone () . to ( device ) for k , v in in_batch . _asdict () . items ()} out = self . forward ( ** in_batch_dict ) logger . debug ( out ) if as_spectrum : spec = DeTensorizer . make_spectrum ( seq = in_batch . seq , mod = in_batch . mods , charge = in_batch . charge , fragment_vector = out . spectra , irt = out . irt , ) out = spec return out elfragmentador.model.ms_transformer_layers Attributes elfragmentador . model . ms_transformer_layers . CONFIG = get_default_config () module-attribute Classes elfragmentador . model . ms_transformer_layers . IRTDecoder ( d_model , dim_feedforward = 224 , nhead = 4 , n_layers = 3 , dropout = 0.05 , final_decoder = 'linear' ) Bases: nn . Module Source code in elfragmentador/model/ms_transformer_layers.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , d_model , dim_feedforward = 224 , nhead = 4 , n_layers = 3 , dropout = 0.05 , final_decoder = \"linear\" , ): super () . __init__ () \"\"\"Decode iRTs. It is technically an encoder-decoder... Args: d_model (int): Number of dimensions to expect as input nhead (int): Number of heads in the attention layers that decode the input. defaults to 4 dim_feedforward (int, optional): Number of hidden dimensions in the FFN that decodes the sequence. Defaults to 224 n_layers (int, optional): dropout to use in the multihead attention. Defaults to 3 \"\"\" self . aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 , ) encoder_layers = nn . TransformerEncoderLayer ( d_model = d_model , nhead = nhead , dim_feedforward = dim_feedforward , dropout = dropout , activation = \"gelu\" , ) self . encoder = nn . TransformerEncoder ( encoder_layer = encoder_layers , num_layers = n_layers ) self . decoder = _LearnableEmbedTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = dim_feedforward , layers = n_layers , dropout = dropout , num_outputs = 1 , final_decoder = final_decoder , ) Attributes aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 ) instance-attribute encoder = nn . TransformerEncoder ( encoder_layer = encoder_layers , num_layers = n_layers ) instance-attribute decoder = _LearnableEmbedTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = dim_feedforward , layers = n_layers , dropout = dropout , num_outputs = 1 , final_decoder = final_decoder ) instance-attribute Functions forward ( seq , mods ) Source code in elfragmentador/model/ms_transformer_layers.py 69 70 71 72 73 74 75 76 77 78 79 80 81 def forward ( self , seq , mods ): # seq [N, S], mods [N, S] trans_encoder_mask = torch . zeros_like ( seq , dtype = torch . float ) trans_encoder_mask = trans_encoder_mask . masked_fill ( seq <= 0 , float ( \"-inf\" ) ) . masked_fill ( seq > 0 , float ( 0.0 )) # mask [N, S] embed_seq = self . aa_embed ( seq = seq , mods = mods ) # [S, N, d_model] memory = self . encoder ( embed_seq , src_key_padding_mask = trans_encoder_mask ) out = self . decoder ( memory , trans_encoder_mask ) return out elfragmentador . model . ms_transformer_layers . PeptideTransformerEncoder ( d_model : int , dropout : float , nhead : int , nhid : int , layers : int ) -> None Bases: torch . nn . Module Source code in elfragmentador/model/ms_transformer_layers.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , d_model : int , dropout : float , nhead : int , nhid : int , layers : int ) -> None : super () . __init__ () # Aminoacid embedding self . aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 , ) # Transformer encoder sections encoder_layers = nn . TransformerEncoderLayer ( d_model = d_model , nhead = nhead , dim_feedforward = nhid , dropout = dropout , activation = \"gelu\" , ) self . encoder = nn . TransformerEncoder ( encoder_layers , layers ) Attributes aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 ) instance-attribute encoder = nn . TransformerEncoder ( encoder_layers , layers ) instance-attribute Functions forward ( seq : Tensor , mods : Tensor ) -> Tensor Source code in elfragmentador/model/ms_transformer_layers.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def forward ( self , seq : Tensor , mods : Tensor ) -> Tensor : # For the mask .... # If a BoolTensor is provided, positions with True are not allowed # to attend while False values will be unchanged <- form the pytorch docs # [1,1,0] # bool [True, True, False] # ~ [False, False, True] # [Attend, Attend, Dont Attend] # seq shape [N, S] trans_encoder_mask = torch . zeros_like ( seq , dtype = torch . float ) trans_encoder_mask = trans_encoder_mask . masked_fill ( seq <= 0 , float ( \"-inf\" ) ) . masked_fill ( seq > 0 , float ( 0.0 )) x = self . aa_embed ( seq = seq , mods = mods ) # x shape [S, N, d_model] trans_encoder_output = self . encoder ( x , src_key_padding_mask = trans_encoder_mask ) # trans_encoder_output shape [S, N, d_model] return trans_encoder_output , trans_encoder_mask elfragmentador . model . ms_transformer_layers . FragmentTransformerDecoder ( d_model : int , nhead : int , nhid : int , layers : int , dropout : float , num_fragments : int , charge_dims_pct : float = 0.05 , nce_dims_pct : float = 0.05 , final_decoder : str = 'linear' ) -> None Bases: _LearnableEmbedTransformerDecoder Source code in elfragmentador/model/ms_transformer_layers.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , d_model : int , nhead : int , nhid : int , layers : int , dropout : float , num_fragments : int , charge_dims_pct : float = 0.05 , nce_dims_pct : float = 0.05 , final_decoder : str = \"linear\" , ) -> None : charge_dims = math . ceil ( d_model * charge_dims_pct ) nce_dims = math . ceil ( d_model * nce_dims_pct ) n_embeds = d_model - ( charge_dims + nce_dims ) super () . __init__ ( d_model = d_model , embed_dims = n_embeds , nhead = nhead , nhid = nhid , layers = layers , dropout = dropout , num_outputs = num_fragments , final_decoder = final_decoder , ) self . charge_encoder = ConcatenationEncoder ( dims_add = charge_dims , max_val = 10.0 , scaling = math . sqrt ( d_model ) ) self . nce_encoder = ConcatenationEncoder ( dims_add = nce_dims , max_val = 100.0 , scaling = math . sqrt ( d_model ) ) self . init_weights () Attributes charge_encoder = ConcatenationEncoder ( dims_add = charge_dims , max_val = 10.0 , scaling = math . sqrt ( d_model )) instance-attribute nce_encoder = ConcatenationEncoder ( dims_add = nce_dims , max_val = 100.0 , scaling = math . sqrt ( d_model )) instance-attribute Functions init_weights () Source code in elfragmentador/model/ms_transformer_layers.py 169 170 171 def init_weights ( self ): initrange = 0.1 nn . init . uniform_ ( self . trans_decoder_embedding . weight , - initrange , initrange ) preprocess_query ( query , charge , nce ) Source code in elfragmentador/model/ms_transformer_layers.py 173 174 175 176 177 178 179 def preprocess_query ( self , query , charge , nce ): # [T, B, E2] trans_decoder_tgt = self . charge_encoder ( query , charge ) # [T, B, E1] trans_decoder_tgt = self . nce_encoder ( trans_decoder_tgt , nce ) # [T, B, E] return trans_decoder_tgt forward ( memory : Tensor , memory_key_padding_mask : Tensor , charge : Tensor , nce : Tensor ) -> Tensor Source code in elfragmentador/model/ms_transformer_layers.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def forward ( self , memory : Tensor , memory_key_padding_mask : Tensor , charge : Tensor , nce : Tensor , ) -> Tensor : trans_decoder_tgt = self . get_learnable_query ( batch_size = charge . size ( 0 )) trans_decoder_tgt = self . preprocess_query ( trans_decoder_tgt , nce = nce , charge = charge ) # [T, B, E] output = self . decoder_forward ( trans_decoder_tgt = trans_decoder_tgt , memory = memory , memory_key_padding_mask = memory_key_padding_mask , ) return output Functions elfragmentador.model.nn_encoding Implements torch models to handle encoding and decoding of positions as well as. learnable embeddings for the aminoacids and ions. Attributes elfragmentador . model . nn_encoding . LiteralFalse = Literal [ False ] module-attribute Classes elfragmentador . model . nn_encoding . ConcatenationEncoder ( dims_add : int , max_val : Union [ float , int ] = 200.0 , static_size : bool = False , scaling = 1 ) -> None Bases: torch . nn . Module ConcatenationEncoder concatenates information into the embedding. Adds information on continuous variables into an embedding by concatenating an n number of dimensions to it. It is meant to add different information to every element in a batch, but the same information (number of dimensions) to every element of a sequence inside an element of the batch. (x[i_1,j,-y:] = x[i_2,j,-y:]) ; being (y) the number of added dimensions. PARAMETER DESCRIPTION dims_add Number of dimensions to add as an encoding TYPE: int max_val maximum expected value of the variable that will be encoded, by default 200.0 TYPE: float DEFAULT: 200.0 static_size Optional ingeter to pass in order to make the size deterministic. This is only required if you want to export your model to torchscript, by default False TYPE: Union [ Literal [False], float ] DEFAULT: False Examples: >>> x1 = torch . zeros (( 5 , 1 , 20 )) >>> x2 = torch . zeros (( 5 , 2 , 20 )) >>> encoder = ConcatenationEncoder ( dims_add = 10 , max_val = 10 ) >>> output = encoder ( x1 , torch . tensor ([[ 7 ]])) >>> output = encoder ( x2 , torch . tensor ([[ 7 ], [ 4 ]])) Source code in elfragmentador/model/nn_encoding.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , dims_add : int , max_val : Union [ float , int ] = 200.0 , static_size : bool = False , scaling = 1 , ) -> None : r \"\"\"ConcatenationEncoder concatenates information into the embedding. Adds information on continuous variables into an embedding by concatenating an n number of dimensions to it. It is meant to add different information to every element in a batch, but the same information (number of dimensions) to every element of a sequence inside an element of the batch. \\(x[i_1,j,-y:] = x[i_2,j,-y:]\\) ; being \\(y\\) the number of added dimensions. Args: dims_add (int): Number of dimensions to add as an encoding max_val (float, optional): maximum expected value of the variable that will be encoded, by default 200.0 static_size (Union[Literal[False], float], optional): Optional ingeter to pass in order to make the size deterministic. This is only required if you want to export your model to torchscript, by default False Examples: >>> x1 = torch.zeros((5, 1, 20)) >>> x2 = torch.zeros((5, 2, 20)) >>> encoder = ConcatenationEncoder(dims_add = 10, max_val=10) >>> output = encoder(x1, torch.tensor([[7]])) >>> output = encoder(x2, torch.tensor([[7], [4]])) \"\"\" super () . __init__ () # pos would be a variable ... div_term = torch . exp ( torch . arange ( 0 , dims_add , 2 ) . float () * ( - math . log ( float ( 2 * max_val )) / ( dims_add )) ) self . register_buffer ( \"div_term\" , div_term ) # TODO add option to make trainable self . static_size = static_size self . dims_add = dims_add self . scaling = scaling Attributes static_size = static_size instance-attribute dims_add = dims_add instance-attribute scaling = scaling instance-attribute Functions forward ( x : Tensor , val : Tensor ) -> Tensor Forward pass thought the encoder. PARAMETER DESCRIPTION x the sequence fed to the encoder model (required). shape is [sequence length, batch size, embed dim] . TYPE: Tensor val value to be encoded into the sequence (required). Shape is [batch size, 1] . TYPE: Tensor RETURNS DESCRIPTION Tensor Tensor (Tensor), Tensor Of shape [sequence length, batch size, embed_dim + added_dims] Examples: >>> x1 = torch . zeros (( 5 , 1 , 20 )) >>> x2 = torch . cat ([ x1 , x1 + 1 ], axis = 1 ) >>> encoder = ConcatenationEncoder ( 10 , max_val = 10 ) >>> output = encoder ( x1 , torch . tensor ([[ 7 ]])) >>> output . shape torch.Size([5, 1, 30]) >>> output = encoder ( x2 , torch . tensor ([[ 7 ], [ 4 ]])) Source code in elfragmentador/model/nn_encoding.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def forward ( self , x : Tensor , val : Tensor ) -> Tensor : \"\"\" Forward pass thought the encoder. Parameters: x (Tensor): the sequence fed to the encoder model (required). shape is **[sequence length, batch size, embed dim]**. val (Tensor): value to be encoded into the sequence (required). Shape is **[batch size, 1]**. Returns: Tensor (Tensor), Of shape **[sequence length, batch size, embed_dim + added_dims]** Examples: >>> x1 = torch.zeros((5, 1, 20)) >>> x2 = torch.cat([x1, x1+1], axis = 1) >>> encoder = ConcatenationEncoder(10, max_val = 10) >>> output = encoder(x1, torch.tensor([[7]])) >>> output.shape torch.Size([5, 1, 30]) >>> output = encoder(x2, torch.tensor([[7], [4]])) \"\"\" e_sin = torch . sin ( val * self . div_term ) e_cos = torch . cos ( val * self . div_term ) e = torch . cat ([ e_sin , e_cos ], axis =- 1 ) assert ( e . shape [ - 1 ] < self . dims_add + 2 ), \"Internal error in concatenation encoder\" e = e [ ... , : self . dims_add ] e = e . unsqueeze ( 0 ) . expand ( x . size ( 0 ), - 1 , - 1 ) / self . scaling x = torch . cat (( x , e ), axis =- 1 ) return x elfragmentador . model . nn_encoding . FourierPositionalEncoding ( d_model : int , max_len : int = 5000 , static_size : Union [ LiteralFalse , int ] = False ) -> None Bases: torch . nn . Module FourierPositionalEncoding adds positional information to tensors. Inject some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies. ({PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model)) ({PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))) where pos is the word position and i is the embed idx) PARAMETER DESCRIPTION d_model the embed dim (required), must be even. TYPE: int max_len the max. length of the incoming sequence (default=5000). TYPE: int DEFAULT: 5000 static_size If it is an integer it is the size of the inputs that will be given, it is used only when tracing the model for torchscript (since torchscript needs fixed length inputs), by default False TYPE: Union [ LiteralFalse , int ] DEFAULT: False Note Therefore encoding are (seq_length, batch, encodings) Examples: >>> posencoder = FourierPositionalEncoding ( 20 , max_len = 20 ) >>> x = torch . ones (( 2 , 1 , 20 )) . float () >>> x . shape torch.Size([2, 1, 20]) >>> posencoder ( x ) . shape torch.Size([2, 1, 20]) Source code in elfragmentador/model/nn_encoding.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def __init__ ( self , d_model : int , max_len : int = 5000 , static_size : Union [ LiteralFalse , int ] = False , ) -> None : r \"\"\"FourierPositionalEncoding adds positional information to tensors. Inject some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies. \\({PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model)\\) \\({PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\\) where pos is the word position and i is the embed idx) Args: d_model (int): the embed dim (required), must be even. max_len (int): the max. length of the incoming sequence (default=5000). static_size (Union[LiteralFalse, int], optional): If it is an integer it is the size of the inputs that will be given, it is used only when tracing the model for torchscript (since torchscript needs fixed length inputs), by default False Note: Therefore encoding are **(seq_length, batch, encodings)** Examples: >>> posencoder = FourierPositionalEncoding(20, max_len=20) >>> x = torch.ones((2,1,20)).float() >>> x.shape torch.Size([2, 1, 20]) >>> posencoder(x).shape torch.Size([2, 1, 20]) \"\"\" super () . __init__ () self . d_model = d_model pe = torch . zeros ( max_len , d_model ) position = torch . arange ( 0 , max_len , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , d_model , 2 , dtype = torch . float ) * ( - math . log ( 10000.0 ) / d_model ) ) pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) pe = pe . unsqueeze ( 0 ) . transpose ( 0 , 1 ) / math . sqrt ( d_model ) # Pe has [shape max_len, 1, d_model] self . register_buffer ( \"pe\" , pe ) self . static_size = static_size Attributes d_model = d_model instance-attribute static_size = static_size instance-attribute Functions forward ( x : Tensor ) -> Tensor Forward pass though the encoder. PARAMETER DESCRIPTION x the sequence fed to the positional encoder model (required). Shape [sequence length, batch size, embed dim] TYPE: Tensor RETURNS DESCRIPTION Tensor Tensor (Tensor), of shape [sequence length, batch size, embed dim] Examples: >>> import pytorch_lightning as pl >>> pl . seed_everything ( 42 ) 42 >>> x = torch . ones (( 4 , 1 , 6 )) . float () >>> pos_encoder = FourierPositionalEncoding ( 6 , max_len = 10 ) >>> output = pos_encoder ( x ) >>> output . shape torch.Size([4, 1, 6]) >>> output tensor([[[...]], [[...]], [[...]], [[...]]]) Source code in elfragmentador/model/nn_encoding.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def forward ( self , x : Tensor ) -> Tensor : r \"\"\"Forward pass though the encoder. Args: x (Tensor): the sequence fed to the positional encoder model (required). Shape **[sequence length, batch size, embed dim]** Returns: Tensor (Tensor), of shape **[sequence length, batch size, embed dim]** Examples: >>> import pytorch_lightning as pl >>> pl.seed_everything(42) 42 >>> x = torch.ones((4,1,6)).float() >>> pos_encoder = FourierPositionalEncoding(6, max_len=10) >>> output = pos_encoder(x) >>> output.shape torch.Size([4, 1, 6]) >>> output tensor([[[...]], [[...]], [[...]], [[...]]]) \"\"\" if self . static_size : end_position = self . static_size else : end_position = x . size ( 0 ) x = x + self . pe [: end_position , :] return x plot_encoding ( ax = None ) Source code in elfragmentador/model/nn_encoding.py 200 201 202 203 def plot_encoding ( self , ax = None ): if ax is None : raise ValueError ( \"Must pass an axis to plot on\" ) ax . imwhow ( self . pe . clone () . detach () . numpy () . squeeze () . numpy ()) elfragmentador . model . nn_encoding . AASequenceEmbedding ( d_model , max_length , aa_names , mod_names , mod_pad_index = 0 , aa_pad_index = 0 ) Bases: torch . nn . Module Source code in elfragmentador/model/nn_encoding.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __init__ ( self , d_model , max_length , aa_names , mod_names , mod_pad_index = 0 , aa_pad_index = 0 ): logger . info ( \"Initializing AASequenceEmbedding\" ) super () . __init__ () # Positional information additions self . aa_names = aa_names self . mod_names = mod_names self . position_embed = FourierPositionalEncoding ( d_model = d_model , max_len = max_length , ) # Aminoacid embedding self . aa_encoder = nn . Embedding ( len ( self . aa_names ), d_model , padding_idx = aa_pad_index ) logger . debug ( \"Aminoacid embedding will use:\" f \" { aa_names [ aa_pad_index ] } as the padding index\" , ) # PTM embedding self . mod_encoder = nn . Embedding ( len ( self . mod_names ), d_model , padding_idx = mod_pad_index ) logger . debug ( \"Modification embedding will use:\" f \" { mod_names [ aa_pad_index ] } as the padding index\" , ) # Weight Initialization self . init_weights () Attributes aa_names = aa_names instance-attribute mod_names = mod_names instance-attribute position_embed = FourierPositionalEncoding ( d_model = d_model , max_len = max_length ) instance-attribute aa_encoder = nn . Embedding ( len ( self . aa_names ), d_model , padding_idx = aa_pad_index ) instance-attribute mod_encoder = nn . Embedding ( len ( self . mod_names ), d_model , padding_idx = mod_pad_index ) instance-attribute Functions init_weights () -> None Source code in elfragmentador/model/nn_encoding.py 242 243 244 245 246 247 248 249 def init_weights ( self ) -> None : logger . info ( \"Initializing weights on AASequenceEmbedding\" ) initrange = 0.1 ptm_initrange = initrange * 0.01 torch . nn . init . uniform_ ( self . aa_encoder . weight , - initrange , initrange ) self . initial_aa_weights = self . aa_encoder . weight . clone () . detach () . cpu () torch . nn . init . uniform_ ( self . mod_encoder . weight , - ptm_initrange , ptm_initrange ) self . initial_mod_weights = self . mod_encoder . weight . clone () . detach () . cpu () forward ( seq , mods ) Source code in elfragmentador/model/nn_encoding.py 251 252 253 254 255 256 257 258 259 260 261 262 def forward ( self , seq , mods ): # seq and mod are [N, S] shaped mods = F . pad ( mods , ( 0 , seq . size ( 1 ) - mods . size ( 1 )), \"constant\" ) seq = self . aa_encoder ( seq . permute ( 1 , 0 )) mods = self . mod_encoder ( mods . permute ( 1 , 0 )) seq = seq + mods # TODO consider if this line is needed, it is used in attention is all you need seq = seq * math . sqrt ( self . aa_encoder . num_embeddings ) seq = self . position_embed ( seq ) return seq as_DataFrames () -> Tuple [ DataFrame , DataFrame ] Returns the weights as data frames. RETURNS DESCRIPTION Tuple [ DataFrame , DataFrame ] Tuple[DataFrame, DataFrame]: A data frame of the aminoacid embeddings and the modification embeddings Examples: >>> from elfragmentador.config import CONFIG >>> embed = AASequenceEmbedding ( ... d_model = 20 , ... aa_names = CONFIG . encoding_aa_order , ... mod_names = CONFIG . encoding_mod_order , ... max_length = 100 ,) >>> aa_embed , mod_embed = embed . as_DataFrames () >>> list ( aa_embed ) ['__missing__', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'c_term', 'n_term'] >>> list ( mod_embed ) [None, '[UNIMOD:4]', '[UNIMOD:35]', '[UNIMOD:21]', '[UNIMOD:121]', '[UNIMOD:737]', '[UNIMOD:1]', '[UNIMOD:34]', '[UNIMOD:36]', '[UNIMOD:37]', '[UNIMOD:354]', '[UNIMOD:7]', '__unknown1__'] Source code in elfragmentador/model/nn_encoding.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def as_DataFrames ( self ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Returns the weights as data frames. Returns: Tuple[DataFrame, DataFrame]: A data frame of the aminoacid embeddings and the modification embeddings Examples: >>> from elfragmentador.config import CONFIG >>> embed = AASequenceEmbedding( ... d_model=20, ... aa_names=CONFIG.encoding_aa_order, ... mod_names=CONFIG.encoding_mod_order, ... max_length=100,) >>> aa_embed, mod_embed = embed.as_DataFrames() >>> list(aa_embed) ['__missing__', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'c_term', 'n_term'] >>> list(mod_embed) [None, '[UNIMOD:4]', '[UNIMOD:35]', '[UNIMOD:21]', '[UNIMOD:121]', '[UNIMOD:737]', '[UNIMOD:1]', '[UNIMOD:34]', '[UNIMOD:36]', '[UNIMOD:37]', '[UNIMOD:354]', '[UNIMOD:7]', '__unknown1__'] \"\"\" df_aa = pd . DataFrame ( data = self . aa_encoder . weight . detach () . numpy () . T ) df_aa . columns = self . aa_names df_mod = pd . DataFrame ( data = self . mod_encoder . weight . detach () . cpu () . numpy () . T ) df_mod . columns = self . mod_names return df_aa , df_mod","title":"PepTransformerModel"},{"location":"API%20reference/PepTransformerModel/#peptransformermodel","text":"","title":"PepTransformerModel"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model","text":"","title":"model"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.LiteralFalse","text":"","title":"LiteralFalse"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model-classes","text":"","title":"Classes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel","text":"Bases: pl . LightningModule PepTransformerModel Predicts retention times and HCD spectra from peptides. init Instantiates the class. Generates a new instance of the PepTransformerModel PARAMETER DESCRIPTION num_decoder_layers int, optional Number of layers in the transformer decoder, by default 6 DEFAULT: 6 num_encoder_layers int, optional Number of laters in the transformer encoder, by default 6 DEFAULT: 6 nhid int, optional Number of dimensions used in the feedforward networks inside the transformer encoder and decoders, by default 2024 DEFAULT: 2024 d_model int, optional Number of features to pass to the transformer encoder. The embedding transforms the input to this input, by default 516 DEFAULT: 516 nhead int, optional Number of multi-attention heads in the transformer, by default 4 DEFAULT: 4 dropout float, optional dropout, by default 0.1 DEFAULT: 0.1 combine_embeds bool, optional Whether the embeddings for modifications and sequences should be shared for irt and fragment predictions TYPE: bool DEFAULT: True combine_encoders bool = True, Whether the transformer encoders for for irt and fragments should be shared. TYPE: bool DEFAULT: True lr float, optional Learning rate, by default 1e-4 DEFAULT: 0.0001 scheduler str, optional What scheduler to use, check the available ones with PepTransformerModel.accepted_schedulers , by default \"plateau\" DEFAULT: 'plateau' lr_ratio Union[float, int], optional For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1. For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn rate would be 1.0. by default 200 DEFAULT: 200 steps_per_epoch None, optional expected number of steps per epoch, used internally to calculate learning rates when using the oncecycle scheduler, by default None DEFAULT: None loss_ratio float, optional The ratio of the spectrum to retention time loss to use when adding before passing to the optimizer. Higher values mean more weight to spectra with respect to the retention time. By default 5 TYPE: float DEFAULT: 5 Source code in elfragmentador/model/__init__.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder : str = \"linear\" , lr : float = 1e-4 , scheduler : str = \"plateau\" , lr_ratio : float | int = 200 , steps_per_epoch : None = None , loss_ratio : float = 5 , * args , ** kwargs , ) -> None : \"\"\" __init__ Instantiates the class. Generates a new instance of the PepTransformerModel Parameters: num_decoder_layers : int, optional Number of layers in the transformer decoder, by default 6 num_encoder_layers : int, optional Number of laters in the transformer encoder, by default 6 nhid : int, optional Number of dimensions used in the feedforward networks inside the transformer encoder and decoders, by default 2024 d_model : int, optional Number of features to pass to the transformer encoder. The embedding transforms the input to this input, by default 516 nhead : int, optional Number of multi-attention heads in the transformer, by default 4 dropout : float, optional dropout, by default 0.1 combine_embeds: bool, optional Whether the embeddings for modifications and sequences should be shared for irt and fragment predictions combine_encoders: bool = True, Whether the transformer encoders for for irt and fragments should be shared. lr : float, optional Learning rate, by default 1e-4 scheduler : str, optional What scheduler to use, check the available ones with `PepTransformerModel.accepted_schedulers`, by default \"plateau\" lr_ratio : Union[float, int], optional For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1. For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn rate would be 1.0. by default 200 steps_per_epoch : None, optional expected number of steps per epoch, used internally to calculate learning rates when using the oncecycle scheduler, by default None loss_ratio: float, optional The ratio of the spectrum to retention time loss to use when adding before passing to the optimizer. Higher values mean more weight to spectra with respect to the retention time. By default 5 \"\"\" super () . __init__ () self . ms2ml_config = CONFIG self . NUM_FRAGMENT_EMBEDDINGS = self . ms2ml_config . num_fragment_embeddings self . save_hyperparameters () logger . info ( f \"num_decoder_layers { num_decoder_layers } \" f \"num_encoder_layers { num_encoder_layers } \" f \"nhid { nhid } d_model { d_model } \" f \"nhead { nhead } dropout { dropout } \" f \"combined embeds { combine_embeds } combined encoders { combine_encoders } \" ) self . main_model = PepTransformerBase ( num_fragments = self . NUM_FRAGMENT_EMBEDDINGS , num_decoder_layers = num_decoder_layers , num_encoder_layers = num_encoder_layers , nhid = nhid , d_model = d_model , nhead = nhead , dropout = dropout , combine_embeds = combine_embeds , combine_encoders = combine_encoders , final_decoder = final_decoder , ) self . metric_calculator = MetricCalculator () self . mse_loss = nn . MSELoss ( reduction = \"none\" ) self . cosine_loss = CosineLoss ( dim = 1 , eps = 1e-8 ) self . angle_loss = SpectralAngleLoss ( dim = 1 , eps = 1e-8 ) # Training related things self . lr = lr assert ( scheduler in self . accepted_schedulers ), f \"Passed scheduler ' { scheduler } is not one of { self . accepted_schedulers } \" self . scheduler = scheduler self . lr_ratio = lr_ratio self . steps_per_epoch = steps_per_epoch self . loss_ratio = loss_ratio self . irt_metric = MissingDataAverager () self . loss_metric = MissingDataAverager () self . spectra_metric = MissingDataAverager () self . spectra_metric2 = MissingDataAverager ()","title":"PepTransformerModel"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.accepted_schedulers","text":"","title":"accepted_schedulers"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.__version__","text":"","title":"__version__"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.ms2ml_config","text":"","title":"ms2ml_config"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.NUM_FRAGMENT_EMBEDDINGS","text":"","title":"NUM_FRAGMENT_EMBEDDINGS"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.main_model","text":"","title":"main_model"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.metric_calculator","text":"","title":"metric_calculator"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.mse_loss","text":"","title":"mse_loss"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.cosine_loss","text":"","title":"cosine_loss"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.angle_loss","text":"","title":"angle_loss"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.lr","text":"","title":"lr"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.scheduler","text":"","title":"scheduler"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.lr_ratio","text":"","title":"lr_ratio"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.steps_per_epoch","text":"","title":"steps_per_epoch"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.loss_ratio","text":"","title":"loss_ratio"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.irt_metric","text":"","title":"irt_metric"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.loss_metric","text":"","title":"loss_metric"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.spectra_metric","text":"","title":"spectra_metric"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.spectra_metric2","text":"","title":"spectra_metric2"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.summarize","text":"Source code in elfragmentador/model/__init__.py 167 168 def summarize ( self , max_depth = 3 ): return summarize ( self , max_depth )","title":"summarize()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.forward","text":"Source code in elfragmentador/model/__init__.py 170 171 172 173 174 175 176 177 def forward ( self , seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor , ): return self . main_model . forward ( seq = seq , mods = mods , charge = charge , nce = nce )","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.predict_from_seq","text":"Source code in elfragmentador/model/__init__.py 179 180 181 182 183 184 185 186 187 188 189 def predict_from_seq ( self , seq : str , nce : float , as_spectrum = False , ) -> PredictionResults | Spectrum : return self . main_model . predict_from_seq ( seq = seq , nce = nce , as_spectrum = as_spectrum , )","title":"predict_from_seq()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.torch_batch_from_seq","text":"Source code in elfragmentador/model/__init__.py 191 192 193 194 @staticmethod def torch_batch_from_seq ( * args , ** kwargs ) -> ForwardBatch : torch_batch_from_seq . __doc__ return torch_batch_from_seq ( * args , ** kwargs )","title":"torch_batch_from_seq()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.to_torchscript","text":"Convert the model to torchscript. Example: model = PepTransformerModel() ts = model.to_torchscript() type(ts) Source code in elfragmentador/model/__init__.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def to_torchscript ( self ): \"\"\" Convert the model to torchscript. Example: >>> model = PepTransformerModel() >>> ts = model.to_torchscript() >>> type(ts) <class 'torch.jit._trace.TopLevelTracedModule'> \"\"\" _fake_input_data_torchscript = self . torch_batch_from_seq ( seq = \"MYM[U:35]DIFIEDPEPTYDE\" , charge = 3 , nce = 27.0 ) backup_calculator = self . metric_calculator self . metric_calculator = None bkp_1 = self . main_model . decoder . nce_encoder . static_size self . main_model . decoder . nce_encoder . static_size = self . NUM_FRAGMENT_EMBEDDINGS bkp_2 = self . main_model . decoder . charge_encoder . static_size self . main_model . decoder . charge_encoder . static_size = ( self . NUM_FRAGMENT_EMBEDDINGS ) script = super () . to_torchscript ( example_inputs = _fake_input_data_torchscript , method = \"trace\" ) self . main_model . decoder . nce_encoder . static_size = bkp_1 self . main_model . decoder . charge_encoder . static_size = bkp_2 self . main_model . metric_calculator = backup_calculator return script","title":"to_torchscript()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.add_model_specific_args","text":"Add_model_specific_args Adds arguments to a parser. It is used to add the command line arguments for the training/generation of the model. PARAMETER DESCRIPTION parser An argparser parser (anything that has the .add_argument method) to which the arguments will be added TYPE: _ArgumentGroup RETURNS DESCRIPTION _ArgumentGroup _ArgumentGroup, the same parser with the added arguments Source code in elfragmentador/model/__init__.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 @staticmethod def add_model_specific_args ( parser : _ArgumentGroup ) -> _ArgumentGroup : \"\"\" Add_model_specific_args Adds arguments to a parser. It is used to add the command line arguments for the training/generation of the model. Args: parser (_ArgumentGroup): An argparser parser (anything that has the `.add_argument` method) to which the arguments will be added Returns: _ArgumentGroup, the same parser with the added arguments \"\"\" parser . add_argument ( \"--num_queries\" , default = 150 , type = int , help = \"Expected encoding length of the spectra\" , ) parser . add_argument ( \"--num_decoder_layers\" , default = 6 , type = int , help = \"Number of sub-encoder-layers in the encoder\" , ) parser . add_argument ( \"--num_encoder_layers\" , default = 6 , type = int , help = \"Number of sub-encoder-layers in the decoder\" , ) parser . add_argument ( \"--nhid\" , default = 1024 , type = int , help = \"Dimension of the feedforward networks\" , ) parser . add_argument ( \"--d_model\" , default = 516 , type = int , help = \"Number of input features to the transformer encoder\" , ) parser . add_argument ( \"--nhead\" , default = 12 , type = int , help = \"Number of attention heads\" ) parser . add_argument ( \"--dropout\" , default = 0.1 , type = float ) parser . add_argument ( \"--combine_embeds\" , action = argparse . BooleanOptionalAction , help = ( \"Whether the embeddings for aminoacid and modifications\" \" should be shared between the irt and fragment sections\" ), ) parser . add_argument ( \"--combine_encoders\" , action = argparse . BooleanOptionalAction , help = ( \"Whether the encoders for aminoacid and modifications\" \" should be shared between the irt and fragment sections\" ), ) parser . add_argument ( \"--final_decoder\" , default = \"mlp\" , type = str , help = ( \"What kind of final layer should the docer have to\" \" output a single number, options are 'mlp' and 'linear'\" ), ) parser . add_argument ( \"--lr\" , default = 1e-4 , type = float ) parser . add_argument ( \"--scheduler\" , default = \"plateau\" , type = str , help = ( \"Scheduler to use during training, \" f \"either of { PepTransformerModel . accepted_schedulers } \" ), ) parser . add_argument ( \"--lr_ratio\" , default = 200.0 , type = float , help = ( \"For cosine annealing: \" \"Ratio of the initial learning rate to use with cosine annealing\" \" for instance a lr or 1 and a ratio of 10 would have a minimum\" \" learning rate of 0.1 \\n \" \"For onecycle: \" \"Ratio of the initial lr and and maximum one, \" \"for instance if lr is 0.1 and ratio is 10, the max learn rate\" \"would be 1.0\" ), ) parser . add_argument ( \"--loss_ratio\" , default = 5.0 , type = float , help = ( \"Ratio between the retention time and the spectrum loss\" \" (higher values mean more weight to the spectra loss\" \" with respect to the retention time loss)\" ), ) return parser","title":"add_model_specific_args()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.configure_scheduler_plateau","text":"Source code in elfragmentador/model/__init__.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 @staticmethod def configure_scheduler_plateau ( optimizer , lr_ratio ): assert lr_ratio < 1 scheduler_dict = { \"scheduler\" : torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer = optimizer , mode = \"min\" , factor = lr_ratio , patience = 2 , verbose = False , ), \"interval\" : \"epoch\" , \"monitor\" : \"val_l\" , } return scheduler_dict","title":"configure_scheduler_plateau()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.configure_scheduler_cosine","text":"Source code in elfragmentador/model/__init__.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 @staticmethod def configure_scheduler_cosine ( optimizer , lr_ratio , min_lr ): assert lr_ratio > 1 scheduler_dict = { \"scheduler\" : CosineAnnealingWarmRestarts ( optimizer = optimizer , T_0 = 1 , T_mult = 2 , eta_min = min_lr , last_epoch =- 1 , verbose = False , ), \"interval\" : \"step\" , } return scheduler_dict","title":"configure_scheduler_cosine()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.configure_scheduler_oncecycle","text":"Source code in elfragmentador/model/__init__.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 @staticmethod def configure_scheduler_oncecycle ( optimizer , lr_ratio , learning_rate , steps_per_epoch , accumulate_grad_batches , max_epochs , ): max_lr = learning_rate * lr_ratio spe = steps_per_epoch // accumulate_grad_batches pct_start = 0.3 logger . info ( f \">> Scheduler setup: max_lr { max_lr } , \" f \"Max Epochs: { max_epochs } , \" f \"Steps per epoch: { steps_per_epoch } , \" f \"SPE (after accum grad batches) { spe } , \" f \"Percent Warmup { pct_start } , \" f \"Accumulate Batches { accumulate_grad_batches } , \" ) scheduler_dict = { \"scheduler\" : torch . optim . lr_scheduler . OneCycleLR ( optimizer = optimizer , max_lr = max_lr , epochs = max_epochs , pct_start = pct_start , steps_per_epoch = spe , ), \"interval\" : \"step\" , } return scheduler_dict","title":"configure_scheduler_oncecycle()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.configure_optimizers","text":"Configure_optimizers COnfigures the optimizers for training. It is internally used by pytorch_lightning during training, so far I implemented 3 options (set when making the module). OneCycleLR seems to give the best results overall in the least amount of time. The only tradeoff that I see is that resuming training does not seem to be really easy. Check the pytorch_lightning documentation to see how this is used in the training loop RETURNS DESCRIPTION tuple [ list [ AdamW ], list [ dict [ str , ReduceLROnPlateau | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , CosineAnnealingWarmRestarts | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , OneCycleLR | str ]]] Two lists, one containing the optimizer and another contining the scheduler. Source code in elfragmentador/model/__init__.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 def configure_optimizers ( self , ) -> ( tuple [ list [ AdamW ], list [ dict [ str , ReduceLROnPlateau | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , CosineAnnealingWarmRestarts | str ]]] | tuple [ list [ AdamW ], list [ dict [ str , OneCycleLR | str ]]] ): \"\"\" Configure_optimizers COnfigures the optimizers for training. It is internally used by pytorch_lightning during training, so far I implemented 3 options (set when making the module). OneCycleLR seems to give the best results overall in the least amount of time. The only tradeoff that I see is that resuming training does not seem to be really easy. Check the pytorch_lightning documentation to see how this is used in the training loop Returns: Two lists, one containing the optimizer and another contining the scheduler. \"\"\" opt = torch . optim . AdamW ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . lr , betas = ( 0.9 , 0.98 ), ) if self . scheduler == \"plateau\" : sched_dict = self . configure_scheduler_plateau ( optimizer = opt , lr_ratio = self . lr_ratio ) elif self . scheduler == \"cosine\" : sched_dict = self . configure_scheduler_cosine ( optimizer = opt , lr_ratio = self . lr_ratio , min_lr = self . lr / self . lr_ratio ) elif self . scheduler == \"onecycle\" : assert self . steps_per_epoch is not None , \"Please set steps_per_epoch\" if self . trainer . max_epochs == 1000 : warnings . warn ( \"Max epochs was 1000, make sure you want this\" ) if self . lr_ratio > 20 : warnings . warn ( f \"Provided LR ratio ' { self . lr_ratio } ' seems a lil high,\" \" make sure you want that for the OneCycleLR scheduler\" ) time . sleep ( 3 ) # just so the user has time to see the message... sched_dict = self . configure_scheduler_oncecycle ( optimizer = opt , lr_ratio = self . lr_ratio , learning_rate = self . lr , steps_per_epoch = self . steps_per_epoch , accumulate_grad_batches = self . trainer . accumulate_grad_batches , max_epochs = self . trainer . max_epochs , ) else : raise ValueError ( \"Scheduler should be one of 'plateau' or 'cosine', passed: \" , self . scheduler , ) # TODO check if using different optimizers for different parts of the # model would work better logger . info ( f \" \\n\\n >>> Setting up schedulers: \\n\\n { sched_dict } \" ) return [ opt ], [ sched_dict ]","title":"configure_optimizers()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.plot_scheduler_lr","text":"Plot the learning rate of the scheduler. This is useful to see how the learning rate changes during training, and to make sure that the scheduler is working as intended. Source code in elfragmentador/model/__init__.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 def plot_scheduler_lr ( self ): \"\"\" Plot the learning rate of the scheduler. This is useful to see how the learning rate changes during training, and to make sure that the scheduler is working as intended. \"\"\" steps_per_epoch = self . steps_per_epoch if steps_per_epoch is None : steps_per_epoch = 1000 try : accumulate_grad_batches = self . trainer . accumulate_grad_batches max_epochs = self . trainer . max_epochs except RuntimeError : accumulate_grad_batches = 1 max_epochs = 10 spe = steps_per_epoch // accumulate_grad_batches optimizer , schedulers = self . configure_optimizers () optimizer = optimizer [ 0 ] scheduler = schedulers [ 0 ][ \"scheduler\" ] xs = list ( range ( spe * max_epochs )) lrs = [] for i in xs : optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) scheduler . step () str_list = uniplot . plot_to_string ( np . log1p ( np . array ( lrs )), xs , title = \"Learning Rate Schedule\" ) plot_str = \" \\n \" . join ( str_list ) logger . info ( f \" \\n\\n { plot_str } \\n\\n \" )","title":"plot_scheduler_lr()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.training_step","text":"See pytorch_lightning documentation. Source code in elfragmentador/model/__init__.py 600 601 602 603 604 605 606 607 608 609 610 611 612 def training_step ( self , batch : TrainBatch , batch_idx : int | None = None ) -> Tensor : \"\"\"See pytorch_lightning documentation.\"\"\" step_out = self . _step ( batch , batch_idx = batch_idx ) log_dict = { \"train_\" + k : v for k , v in step_out . items ()} log_dict . update ({ \"LR\" : self . trainer . optimizers [ 0 ] . param_groups [ 0 ][ \"lr\" ]}) self . log_dict ( log_dict , prog_bar = True , # reduce_fx=nanmean, ) return step_out [ \"l\" ]","title":"training_step()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.on_train_start","text":"Source code in elfragmentador/model/__init__.py 614 615 616 617 def on_train_start ( self ) -> None : logger . info ( \"Weights before the start of the training epoch:\" ) logger . info ( copy . deepcopy ( self . state_dict ())) return super () . on_train_start ()","title":"on_train_start()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.validation_step","text":"See pytorch_lightning documentation. Source code in elfragmentador/model/__init__.py 619 620 621 622 623 624 625 626 627 628 629 630 def validation_step ( self , batch : TrainBatch , batch_idx : int | None = None ) -> Tensor : \"\"\"See pytorch_lightning documentation.\"\"\" step_out = self . _step ( batch , batch_idx = batch_idx ) self . irt_metric . update ( step_out [ \"irt_l\" ]) self . loss_metric . update ( step_out [ \"l\" ]) self . spectra_metric . update ( step_out [ \"spec_l\" ]) self . spectra_metric2 . update ( step_out [ \"spec_l2\" ]) return step_out [ \"l\" ]","title":"validation_step()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.validation_epoch_end","text":"See pytorch lightning documentation. Source code in elfragmentador/model/__init__.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def validation_epoch_end ( self , outputs : list [ Tensor ]) -> list [ Tensor ]: \"\"\"See pytorch lightning documentation.\"\"\" log_dict = { \"val_irt_l\" : self . irt_metric . compute (), \"val_l\" : self . loss_metric . compute (), \"val_spec_l\" : self . spectra_metric . compute (), \"val_spec_l2\" : self . spectra_metric2 . compute (), } self . log_dict ( log_dict , prog_bar = True , ) self . irt_metric . reset () self . loss_metric . reset () self . spectra_metric . reset () self . spectra_metric2 . reset () return super () . validation_epoch_end ( outputs )","title":"validation_epoch_end()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.test_step","text":"Source code in elfragmentador/model/__init__.py 686 687 688 689 690 def test_step ( self , batch , batch_idx : int | None = None ) -> tuple [ dict [ str , Tensor ], PredictionResults ]: losses , pred_out = self . _evaluation_step ( batch = batch , batch_idx = batch_idx ) return losses , pred_out . irt , batch . irt","title":"test_step()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.test_epoch_end","text":"Source code in elfragmentador/model/__init__.py 692 693 694 695 def test_epoch_end ( self , results : list ): self . metric_calculator . trainer = self . trainer self . metric_calculator . log_dict = self . log_dict return self . metric_calculator . test_epoch_end ( results )","title":"test_epoch_end()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.predict_step","text":"Source code in elfragmentador/model/__init__.py 697 698 699 700 701 702 def predict_step ( self , batch : TrainBatch , batch_idx : int | None = None ): yhat_irt , yhat_spectra = self . forward ( seq = batch . seq , mods = batch . mods , charge = batch . charge , nce = batch . nce ) pred_out = PredictionResults ( irt = yhat_irt , spectra = torch . relu ( yhat_spectra )) return pred_out","title":"predict_step()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.on_after_backward","text":"Source code in elfragmentador/model/__init__.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 def on_after_backward ( self ): msg = [] global_step = self . global_step if ( global_step % 50 ) == 0 : for name , param in self . named_parameters (): if \"weight\" in name and \"norm\" not in name : if param . requires_grad : try : if param . grad is None : raise AttributeError if any ( x in name for x in [ \"aa_encoder.weight\" , \"mod_encoder.weight\" , \"trans_decoder_embedding.weight\" , ] ): val = param . grad . abs () . mean () if torch . any ( torch . isnan ( val )): logger . error ( f \"nan mean gradient for { name } : { param . grad } \" ) self . log ( name , val , prog_bar = True , on_step = True ) except AttributeError : msg . append ( name ) except ValueError : msg . append ( name ) if len ( msg ) > 0 : logger . warning ( \" \" . join ( msg ) + \"Did not have gradients in step {global_step} \" )","title":"on_after_backward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.on_train_epoch_end","text":"Source code in elfragmentador/model/__init__.py 738 739 740 def on_train_epoch_end ( self ) -> None : evaluate_landmark_rt ( self ) return super () . on_train_epoch_end ()","title":"on_train_epoch_end()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.PepTransformerModel.load_from_checkpoint","text":"Source code in elfragmentador/model/__init__.py 742 743 744 745 746 @classmethod def load_from_checkpoint ( cls , * args , ** kwargs ): mod = super () . load_from_checkpoint ( * args , ** kwargs ) evaluate_landmark_rt ( mod ) return mod","title":"load_from_checkpoint()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.evaluate_landmark_rt","text":"Checks the prediction of the model on the iRT peptides. Predicts all the procal and Biognosys iRT peptides and checks the correlation of the theoretical iRT values and the predicted ones PARAMETER DESCRIPTION model PepTransformerModel A model to test the predictions on TYPE: PepTransformerModel Source code in elfragmentador/model/__init__.py 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 def evaluate_landmark_rt ( model : PepTransformerModel ): \"\"\"Checks the prediction of the model on the iRT peptides. Predicts all the procal and Biognosys iRT peptides and checks the correlation of the theoretical iRT values and the predicted ones Parameters: model: PepTransformerModel A model to test the predictions on \"\"\" model . eval () real_rt = [] pred_rt = [] for seq , desc in IRT_PEPTIDES . items (): with torch . no_grad (): out = model . predict_from_seq ( f \" { seq } /2\" , 25 ) pred_rt . append ( out . irt . clone () . cpu () . numpy ()) real_rt . append ( np . array ( desc [ \"irt\" ])) fit = polyfit ( np . array ( real_rt ) . flatten (), np . array ( pred_rt ) . flatten ()) logger . info ( fit ) plot_str = uniplot . plot_to_string ( xs = np . array ( real_rt ) . flatten (), ys = np . array ( pred_rt ) . flatten (), title = \"Prediction vs real iRT of biognosys and procal peptides\" , ) logger . info ( \" \\n \" + \" \\n \" . join ( plot_str ), \" \\n \" ) return fit , plot_str","title":"evaluate_landmark_rt()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer","text":"","title":"peptransformer"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer-classes","text":"","title":"Classes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase","text":"Bases: nn . Module Source code in elfragmentador/model/peptransformer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , num_fragments , num_decoder_layers : int = 6 , num_encoder_layers : int = 6 , nhid : int = 2024 , d_model : int = 516 , nhead : int = 4 , dropout : float = 0.1 , combine_embeds : bool = True , combine_encoders : bool = True , final_decoder = \"linear\" , ) -> None : super () . __init__ () # Peptide encoder self . encoder = PeptideTransformerEncoder ( d_model = d_model , dropout = dropout , nhead = nhead , nhid = nhid , layers = num_encoder_layers , ) # Peptide decoder self . decoder = FragmentTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = nhid , layers = num_decoder_layers , dropout = dropout , num_fragments = num_fragments , final_decoder = final_decoder , ) self . irt_decoder = IRTDecoder ( d_model = d_model , dim_feedforward = nhid , nhead = nhead , n_layers = num_encoder_layers , dropout = dropout , final_decoder = final_decoder , ) if combine_embeds : self . irt_decoder . aa_embed = self . encoder . aa_embed if combine_encoders : self . irt_decoder . encoder = self . encoder . encoder","title":"PepTransformerBase"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase.encoder","text":"","title":"encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase.decoder","text":"","title":"decoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase.irt_decoder","text":"","title":"irt_decoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase.forward","text":"Forward Generate predictions. Privides the function for the forward pass to the model. PARAMETER DESCRIPTION seq Encoded pepide sequence [B, L] (view details) TYPE: Tensor mods Encoded modification sequence [B, L], by default None TYPE: Tensor nce float Tensor with the charges [B, 1] TYPE: Tensor charge long Tensor with the charges [B, 1], by default None TYPE: Tensor Details seq: The peptide is encoded as integers for the aminoacid. \"AAA\" encoded for a max length of 5 would be torch.Tensor([ 1, 1, 1, 0, 0]).long() nce: Normalized collision energy to use during the prediction. charge: A tensor corresponding to the charges of each of the peptide precursors (long) mods: Modifications encoded as integers Source code in elfragmentador/model/peptransformer.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def forward ( self , seq : Tensor , mods : Tensor , charge : Tensor , nce : Tensor , ) -> PredictionResults : \"\"\" Forward Generate predictions. Privides the function for the forward pass to the model. Parameters: seq (Tensor): Encoded pepide sequence [B, L] (view details) mods (Tensor): Encoded modification sequence [B, L], by default None nce (Tensor): float Tensor with the charges [B, 1] charge (Tensor): long Tensor with the charges [B, 1], by default None Details: seq: The peptide is encoded as integers for the aminoacid. \"AAA\" encoded for a max length of 5 would be torch.Tensor([ 1, 1, 1, 0, 0]).long() nce: Normalized collision energy to use during the prediction. charge: A tensor corresponding to the charges of each of the peptide precursors (long) mods: Modifications encoded as integers \"\"\" trans_encoder_output , mem_mask = self . encoder ( seq = seq , mods = mods ) rt_output = self . irt_decoder ( seq = seq , mods = mods ) spectra_output = self . decoder ( memory = trans_encoder_output , charge = charge , nce = nce , memory_key_padding_mask = mem_mask , ) return PredictionResults ( irt = rt_output , spectra = spectra_output )","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.peptransformer.PepTransformerBase.predict_from_seq","text":"Predict_from_seq Predicts spectra from a sequence as a string. Utility method that gets a sequence as a string, encodes it internally to the correct input form and outputs the predicted spectra. Note that the spectra is not decoded as an output, please check elfragmentador.encoding_decoding.decode_fragment_tensor for the decoding. The irt is scaled by 100 and is in the Biognosys scale. TODO: consider if the output should be decoded ... PARAMETER DESCRIPTION seq Sequence to use for prediction, supports modifications in the form of S[PHOSPHO], S[+80] and T[181] TYPE: str nce Normalized collision energy to use during the prediction TYPE: float as_spectrum Wether to return a Spectrum object instead of the raw tensor predictions (Default value = False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION PredictionResults A named tuple with two named results; irt and spectra TYPE: PredictionResults | AnnotatedPeptideSpectrum Spectrum A spectrum object with the predicted spectrum TYPE: PredictionResults | AnnotatedPeptideSpectrum Examples: >>> import pytorch_lightning as pl >>> from elfragmentador.config import CONFIG >>> pl . seed_everything ( 42 ) 42 >>> my_model = PepTransformerBase ( num_fragments = CONFIG . num_fragment_embeddings ) # Or load the model from a checkpoint >>> _ = my_model . eval () >>> my_model . predict_from_seq ( \"MYPEPT[U:21]IDEK/3\" , 27 ) PredictionResults(irt=tensor(...), spectra=tensor([...])) >>> out = my_model . predict_from_seq ( \"MYPEPT[U:21]IDEK/3\" , 27 , as_spectrum = True ) >>> type ( out ) <class 'ms2ml.spectrum.AnnotatedPeptideSpectrum'> >>> # my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) Source code in elfragmentador/model/peptransformer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @torch . no_grad () def predict_from_seq ( self , seq : str , nce : float , as_spectrum = False , ) -> PredictionResults | AnnotatedPeptideSpectrum : \"\"\" Predict_from_seq Predicts spectra from a sequence as a string. Utility method that gets a sequence as a string, encodes it internally to the correct input form and outputs the predicted spectra. Note that the spectra is not decoded as an output, please check `elfragmentador.encoding_decoding.decode_fragment_tensor` for the decoding. The irt is scaled by 100 and is in the Biognosys scale. TODO: consider if the output should be decoded ... Parameters: seq (str): Sequence to use for prediction, supports modifications in the form of S[PHOSPHO], S[+80] and T[181] nce (float): Normalized collision energy to use during the prediction as_spectrum (bool, optional): Wether to return a Spectrum object instead of the raw tensor predictions (Default value = False) Returns: PredictionResults: A named tuple with two named results; irt and spectra Spectrum: A spectrum object with the predicted spectrum Examples: >>> import pytorch_lightning as pl >>> from elfragmentador.config import CONFIG >>> pl.seed_everything(42) 42 >>> my_model = PepTransformerBase(num_fragments=CONFIG.num_fragment_embeddings) # Or load the model from a checkpoint >>> _ = my_model.eval() >>> my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) PredictionResults(irt=tensor(...), spectra=tensor([...])) >>> out = my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27, \\ as_spectrum=True) >>> type(out) <class 'ms2ml.spectrum.AnnotatedPeptideSpectrum'> >>> # my_model.predict_from_seq(\"MYPEPT[U:21]IDEK/3\", 27) \"\"\" # noqa in_batch = Tensorizer () . convert_string ( data = seq , nce = nce ) device = next ( self . parameters ()) . device in_batch_dict = { k : v . clone () . to ( device ) for k , v in in_batch . _asdict () . items ()} out = self . forward ( ** in_batch_dict ) logger . debug ( out ) if as_spectrum : spec = DeTensorizer . make_spectrum ( seq = in_batch . seq , mod = in_batch . mods , charge = in_batch . charge , fragment_vector = out . spectra , irt = out . irt , ) out = spec return out","title":"predict_from_seq()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers","text":"","title":"ms_transformer_layers"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.CONFIG","text":"","title":"CONFIG"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers-classes","text":"","title":"Classes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder","text":"Bases: nn . Module Source code in elfragmentador/model/ms_transformer_layers.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , d_model , dim_feedforward = 224 , nhead = 4 , n_layers = 3 , dropout = 0.05 , final_decoder = \"linear\" , ): super () . __init__ () \"\"\"Decode iRTs. It is technically an encoder-decoder... Args: d_model (int): Number of dimensions to expect as input nhead (int): Number of heads in the attention layers that decode the input. defaults to 4 dim_feedforward (int, optional): Number of hidden dimensions in the FFN that decodes the sequence. Defaults to 224 n_layers (int, optional): dropout to use in the multihead attention. Defaults to 3 \"\"\" self . aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 , ) encoder_layers = nn . TransformerEncoderLayer ( d_model = d_model , nhead = nhead , dim_feedforward = dim_feedforward , dropout = dropout , activation = \"gelu\" , ) self . encoder = nn . TransformerEncoder ( encoder_layer = encoder_layers , num_layers = n_layers ) self . decoder = _LearnableEmbedTransformerDecoder ( d_model = d_model , nhead = nhead , nhid = dim_feedforward , layers = n_layers , dropout = dropout , num_outputs = 1 , final_decoder = final_decoder , )","title":"IRTDecoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder.aa_embed","text":"","title":"aa_embed"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder.encoder","text":"","title":"encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder.decoder","text":"","title":"decoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.IRTDecoder.forward","text":"Source code in elfragmentador/model/ms_transformer_layers.py 69 70 71 72 73 74 75 76 77 78 79 80 81 def forward ( self , seq , mods ): # seq [N, S], mods [N, S] trans_encoder_mask = torch . zeros_like ( seq , dtype = torch . float ) trans_encoder_mask = trans_encoder_mask . masked_fill ( seq <= 0 , float ( \"-inf\" ) ) . masked_fill ( seq > 0 , float ( 0.0 )) # mask [N, S] embed_seq = self . aa_embed ( seq = seq , mods = mods ) # [S, N, d_model] memory = self . encoder ( embed_seq , src_key_padding_mask = trans_encoder_mask ) out = self . decoder ( memory , trans_encoder_mask ) return out","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder","text":"Bases: torch . nn . Module Source code in elfragmentador/model/ms_transformer_layers.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , d_model : int , dropout : float , nhead : int , nhid : int , layers : int ) -> None : super () . __init__ () # Aminoacid embedding self . aa_embed = AASequenceEmbedding ( d_model = d_model , aa_names = CONFIG . encoding_aa_order , mod_names = CONFIG . encoding_mod_order , max_length = 100 , ) # Transformer encoder sections encoder_layers = nn . TransformerEncoderLayer ( d_model = d_model , nhead = nhead , dim_feedforward = nhid , dropout = dropout , activation = \"gelu\" , ) self . encoder = nn . TransformerEncoder ( encoder_layers , layers )","title":"PeptideTransformerEncoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder.aa_embed","text":"","title":"aa_embed"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder.encoder","text":"","title":"encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.PeptideTransformerEncoder.forward","text":"Source code in elfragmentador/model/ms_transformer_layers.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def forward ( self , seq : Tensor , mods : Tensor ) -> Tensor : # For the mask .... # If a BoolTensor is provided, positions with True are not allowed # to attend while False values will be unchanged <- form the pytorch docs # [1,1,0] # bool [True, True, False] # ~ [False, False, True] # [Attend, Attend, Dont Attend] # seq shape [N, S] trans_encoder_mask = torch . zeros_like ( seq , dtype = torch . float ) trans_encoder_mask = trans_encoder_mask . masked_fill ( seq <= 0 , float ( \"-inf\" ) ) . masked_fill ( seq > 0 , float ( 0.0 )) x = self . aa_embed ( seq = seq , mods = mods ) # x shape [S, N, d_model] trans_encoder_output = self . encoder ( x , src_key_padding_mask = trans_encoder_mask ) # trans_encoder_output shape [S, N, d_model] return trans_encoder_output , trans_encoder_mask","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder","text":"Bases: _LearnableEmbedTransformerDecoder Source code in elfragmentador/model/ms_transformer_layers.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , d_model : int , nhead : int , nhid : int , layers : int , dropout : float , num_fragments : int , charge_dims_pct : float = 0.05 , nce_dims_pct : float = 0.05 , final_decoder : str = \"linear\" , ) -> None : charge_dims = math . ceil ( d_model * charge_dims_pct ) nce_dims = math . ceil ( d_model * nce_dims_pct ) n_embeds = d_model - ( charge_dims + nce_dims ) super () . __init__ ( d_model = d_model , embed_dims = n_embeds , nhead = nhead , nhid = nhid , layers = layers , dropout = dropout , num_outputs = num_fragments , final_decoder = final_decoder , ) self . charge_encoder = ConcatenationEncoder ( dims_add = charge_dims , max_val = 10.0 , scaling = math . sqrt ( d_model ) ) self . nce_encoder = ConcatenationEncoder ( dims_add = nce_dims , max_val = 100.0 , scaling = math . sqrt ( d_model ) ) self . init_weights ()","title":"FragmentTransformerDecoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder.charge_encoder","text":"","title":"charge_encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder.nce_encoder","text":"","title":"nce_encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder.init_weights","text":"Source code in elfragmentador/model/ms_transformer_layers.py 169 170 171 def init_weights ( self ): initrange = 0.1 nn . init . uniform_ ( self . trans_decoder_embedding . weight , - initrange , initrange )","title":"init_weights()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder.preprocess_query","text":"Source code in elfragmentador/model/ms_transformer_layers.py 173 174 175 176 177 178 179 def preprocess_query ( self , query , charge , nce ): # [T, B, E2] trans_decoder_tgt = self . charge_encoder ( query , charge ) # [T, B, E1] trans_decoder_tgt = self . nce_encoder ( trans_decoder_tgt , nce ) # [T, B, E] return trans_decoder_tgt","title":"preprocess_query()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers.FragmentTransformerDecoder.forward","text":"Source code in elfragmentador/model/ms_transformer_layers.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def forward ( self , memory : Tensor , memory_key_padding_mask : Tensor , charge : Tensor , nce : Tensor , ) -> Tensor : trans_decoder_tgt = self . get_learnable_query ( batch_size = charge . size ( 0 )) trans_decoder_tgt = self . preprocess_query ( trans_decoder_tgt , nce = nce , charge = charge ) # [T, B, E] output = self . decoder_forward ( trans_decoder_tgt = trans_decoder_tgt , memory = memory , memory_key_padding_mask = memory_key_padding_mask , ) return output","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.ms_transformer_layers-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding","text":"Implements torch models to handle encoding and decoding of positions as well as. learnable embeddings for the aminoacids and ions.","title":"nn_encoding"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.LiteralFalse","text":"","title":"LiteralFalse"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding-classes","text":"","title":"Classes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder","text":"Bases: torch . nn . Module ConcatenationEncoder concatenates information into the embedding. Adds information on continuous variables into an embedding by concatenating an n number of dimensions to it. It is meant to add different information to every element in a batch, but the same information (number of dimensions) to every element of a sequence inside an element of the batch. (x[i_1,j,-y:] = x[i_2,j,-y:]) ; being (y) the number of added dimensions. PARAMETER DESCRIPTION dims_add Number of dimensions to add as an encoding TYPE: int max_val maximum expected value of the variable that will be encoded, by default 200.0 TYPE: float DEFAULT: 200.0 static_size Optional ingeter to pass in order to make the size deterministic. This is only required if you want to export your model to torchscript, by default False TYPE: Union [ Literal [False], float ] DEFAULT: False Examples: >>> x1 = torch . zeros (( 5 , 1 , 20 )) >>> x2 = torch . zeros (( 5 , 2 , 20 )) >>> encoder = ConcatenationEncoder ( dims_add = 10 , max_val = 10 ) >>> output = encoder ( x1 , torch . tensor ([[ 7 ]])) >>> output = encoder ( x2 , torch . tensor ([[ 7 ], [ 4 ]])) Source code in elfragmentador/model/nn_encoding.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , dims_add : int , max_val : Union [ float , int ] = 200.0 , static_size : bool = False , scaling = 1 , ) -> None : r \"\"\"ConcatenationEncoder concatenates information into the embedding. Adds information on continuous variables into an embedding by concatenating an n number of dimensions to it. It is meant to add different information to every element in a batch, but the same information (number of dimensions) to every element of a sequence inside an element of the batch. \\(x[i_1,j,-y:] = x[i_2,j,-y:]\\) ; being \\(y\\) the number of added dimensions. Args: dims_add (int): Number of dimensions to add as an encoding max_val (float, optional): maximum expected value of the variable that will be encoded, by default 200.0 static_size (Union[Literal[False], float], optional): Optional ingeter to pass in order to make the size deterministic. This is only required if you want to export your model to torchscript, by default False Examples: >>> x1 = torch.zeros((5, 1, 20)) >>> x2 = torch.zeros((5, 2, 20)) >>> encoder = ConcatenationEncoder(dims_add = 10, max_val=10) >>> output = encoder(x1, torch.tensor([[7]])) >>> output = encoder(x2, torch.tensor([[7], [4]])) \"\"\" super () . __init__ () # pos would be a variable ... div_term = torch . exp ( torch . arange ( 0 , dims_add , 2 ) . float () * ( - math . log ( float ( 2 * max_val )) / ( dims_add )) ) self . register_buffer ( \"div_term\" , div_term ) # TODO add option to make trainable self . static_size = static_size self . dims_add = dims_add self . scaling = scaling","title":"ConcatenationEncoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder.static_size","text":"","title":"static_size"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder.dims_add","text":"","title":"dims_add"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder.scaling","text":"","title":"scaling"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.ConcatenationEncoder.forward","text":"Forward pass thought the encoder. PARAMETER DESCRIPTION x the sequence fed to the encoder model (required). shape is [sequence length, batch size, embed dim] . TYPE: Tensor val value to be encoded into the sequence (required). Shape is [batch size, 1] . TYPE: Tensor RETURNS DESCRIPTION Tensor Tensor (Tensor), Tensor Of shape [sequence length, batch size, embed_dim + added_dims] Examples: >>> x1 = torch . zeros (( 5 , 1 , 20 )) >>> x2 = torch . cat ([ x1 , x1 + 1 ], axis = 1 ) >>> encoder = ConcatenationEncoder ( 10 , max_val = 10 ) >>> output = encoder ( x1 , torch . tensor ([[ 7 ]])) >>> output . shape torch.Size([5, 1, 30]) >>> output = encoder ( x2 , torch . tensor ([[ 7 ], [ 4 ]])) Source code in elfragmentador/model/nn_encoding.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def forward ( self , x : Tensor , val : Tensor ) -> Tensor : \"\"\" Forward pass thought the encoder. Parameters: x (Tensor): the sequence fed to the encoder model (required). shape is **[sequence length, batch size, embed dim]**. val (Tensor): value to be encoded into the sequence (required). Shape is **[batch size, 1]**. Returns: Tensor (Tensor), Of shape **[sequence length, batch size, embed_dim + added_dims]** Examples: >>> x1 = torch.zeros((5, 1, 20)) >>> x2 = torch.cat([x1, x1+1], axis = 1) >>> encoder = ConcatenationEncoder(10, max_val = 10) >>> output = encoder(x1, torch.tensor([[7]])) >>> output.shape torch.Size([5, 1, 30]) >>> output = encoder(x2, torch.tensor([[7], [4]])) \"\"\" e_sin = torch . sin ( val * self . div_term ) e_cos = torch . cos ( val * self . div_term ) e = torch . cat ([ e_sin , e_cos ], axis =- 1 ) assert ( e . shape [ - 1 ] < self . dims_add + 2 ), \"Internal error in concatenation encoder\" e = e [ ... , : self . dims_add ] e = e . unsqueeze ( 0 ) . expand ( x . size ( 0 ), - 1 , - 1 ) / self . scaling x = torch . cat (( x , e ), axis =- 1 ) return x","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding","text":"Bases: torch . nn . Module FourierPositionalEncoding adds positional information to tensors. Inject some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies. ({PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model)) ({PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))) where pos is the word position and i is the embed idx) PARAMETER DESCRIPTION d_model the embed dim (required), must be even. TYPE: int max_len the max. length of the incoming sequence (default=5000). TYPE: int DEFAULT: 5000 static_size If it is an integer it is the size of the inputs that will be given, it is used only when tracing the model for torchscript (since torchscript needs fixed length inputs), by default False TYPE: Union [ LiteralFalse , int ] DEFAULT: False Note Therefore encoding are (seq_length, batch, encodings) Examples: >>> posencoder = FourierPositionalEncoding ( 20 , max_len = 20 ) >>> x = torch . ones (( 2 , 1 , 20 )) . float () >>> x . shape torch.Size([2, 1, 20]) >>> posencoder ( x ) . shape torch.Size([2, 1, 20]) Source code in elfragmentador/model/nn_encoding.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def __init__ ( self , d_model : int , max_len : int = 5000 , static_size : Union [ LiteralFalse , int ] = False , ) -> None : r \"\"\"FourierPositionalEncoding adds positional information to tensors. Inject some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies. \\({PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model)\\) \\({PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\\) where pos is the word position and i is the embed idx) Args: d_model (int): the embed dim (required), must be even. max_len (int): the max. length of the incoming sequence (default=5000). static_size (Union[LiteralFalse, int], optional): If it is an integer it is the size of the inputs that will be given, it is used only when tracing the model for torchscript (since torchscript needs fixed length inputs), by default False Note: Therefore encoding are **(seq_length, batch, encodings)** Examples: >>> posencoder = FourierPositionalEncoding(20, max_len=20) >>> x = torch.ones((2,1,20)).float() >>> x.shape torch.Size([2, 1, 20]) >>> posencoder(x).shape torch.Size([2, 1, 20]) \"\"\" super () . __init__ () self . d_model = d_model pe = torch . zeros ( max_len , d_model ) position = torch . arange ( 0 , max_len , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , d_model , 2 , dtype = torch . float ) * ( - math . log ( 10000.0 ) / d_model ) ) pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) pe = pe . unsqueeze ( 0 ) . transpose ( 0 , 1 ) / math . sqrt ( d_model ) # Pe has [shape max_len, 1, d_model] self . register_buffer ( \"pe\" , pe ) self . static_size = static_size","title":"FourierPositionalEncoding"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding.d_model","text":"","title":"d_model"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding.static_size","text":"","title":"static_size"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding.forward","text":"Forward pass though the encoder. PARAMETER DESCRIPTION x the sequence fed to the positional encoder model (required). Shape [sequence length, batch size, embed dim] TYPE: Tensor RETURNS DESCRIPTION Tensor Tensor (Tensor), of shape [sequence length, batch size, embed dim] Examples: >>> import pytorch_lightning as pl >>> pl . seed_everything ( 42 ) 42 >>> x = torch . ones (( 4 , 1 , 6 )) . float () >>> pos_encoder = FourierPositionalEncoding ( 6 , max_len = 10 ) >>> output = pos_encoder ( x ) >>> output . shape torch.Size([4, 1, 6]) >>> output tensor([[[...]], [[...]], [[...]], [[...]]]) Source code in elfragmentador/model/nn_encoding.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def forward ( self , x : Tensor ) -> Tensor : r \"\"\"Forward pass though the encoder. Args: x (Tensor): the sequence fed to the positional encoder model (required). Shape **[sequence length, batch size, embed dim]** Returns: Tensor (Tensor), of shape **[sequence length, batch size, embed dim]** Examples: >>> import pytorch_lightning as pl >>> pl.seed_everything(42) 42 >>> x = torch.ones((4,1,6)).float() >>> pos_encoder = FourierPositionalEncoding(6, max_len=10) >>> output = pos_encoder(x) >>> output.shape torch.Size([4, 1, 6]) >>> output tensor([[[...]], [[...]], [[...]], [[...]]]) \"\"\" if self . static_size : end_position = self . static_size else : end_position = x . size ( 0 ) x = x + self . pe [: end_position , :] return x","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.FourierPositionalEncoding.plot_encoding","text":"Source code in elfragmentador/model/nn_encoding.py 200 201 202 203 def plot_encoding ( self , ax = None ): if ax is None : raise ValueError ( \"Must pass an axis to plot on\" ) ax . imwhow ( self . pe . clone () . detach () . numpy () . squeeze () . numpy ())","title":"plot_encoding()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding","text":"Bases: torch . nn . Module Source code in elfragmentador/model/nn_encoding.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __init__ ( self , d_model , max_length , aa_names , mod_names , mod_pad_index = 0 , aa_pad_index = 0 ): logger . info ( \"Initializing AASequenceEmbedding\" ) super () . __init__ () # Positional information additions self . aa_names = aa_names self . mod_names = mod_names self . position_embed = FourierPositionalEncoding ( d_model = d_model , max_len = max_length , ) # Aminoacid embedding self . aa_encoder = nn . Embedding ( len ( self . aa_names ), d_model , padding_idx = aa_pad_index ) logger . debug ( \"Aminoacid embedding will use:\" f \" { aa_names [ aa_pad_index ] } as the padding index\" , ) # PTM embedding self . mod_encoder = nn . Embedding ( len ( self . mod_names ), d_model , padding_idx = mod_pad_index ) logger . debug ( \"Modification embedding will use:\" f \" { mod_names [ aa_pad_index ] } as the padding index\" , ) # Weight Initialization self . init_weights ()","title":"AASequenceEmbedding"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding-attributes","text":"","title":"Attributes"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.aa_names","text":"","title":"aa_names"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.mod_names","text":"","title":"mod_names"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.position_embed","text":"","title":"position_embed"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.aa_encoder","text":"","title":"aa_encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.mod_encoder","text":"","title":"mod_encoder"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding-functions","text":"","title":"Functions"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.init_weights","text":"Source code in elfragmentador/model/nn_encoding.py 242 243 244 245 246 247 248 249 def init_weights ( self ) -> None : logger . info ( \"Initializing weights on AASequenceEmbedding\" ) initrange = 0.1 ptm_initrange = initrange * 0.01 torch . nn . init . uniform_ ( self . aa_encoder . weight , - initrange , initrange ) self . initial_aa_weights = self . aa_encoder . weight . clone () . detach () . cpu () torch . nn . init . uniform_ ( self . mod_encoder . weight , - ptm_initrange , ptm_initrange ) self . initial_mod_weights = self . mod_encoder . weight . clone () . detach () . cpu ()","title":"init_weights()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.forward","text":"Source code in elfragmentador/model/nn_encoding.py 251 252 253 254 255 256 257 258 259 260 261 262 def forward ( self , seq , mods ): # seq and mod are [N, S] shaped mods = F . pad ( mods , ( 0 , seq . size ( 1 ) - mods . size ( 1 )), \"constant\" ) seq = self . aa_encoder ( seq . permute ( 1 , 0 )) mods = self . mod_encoder ( mods . permute ( 1 , 0 )) seq = seq + mods # TODO consider if this line is needed, it is used in attention is all you need seq = seq * math . sqrt ( self . aa_encoder . num_embeddings ) seq = self . position_embed ( seq ) return seq","title":"forward()"},{"location":"API%20reference/PepTransformerModel/#elfragmentador.model.nn_encoding.AASequenceEmbedding.as_DataFrames","text":"Returns the weights as data frames. RETURNS DESCRIPTION Tuple [ DataFrame , DataFrame ] Tuple[DataFrame, DataFrame]: A data frame of the aminoacid embeddings and the modification embeddings Examples: >>> from elfragmentador.config import CONFIG >>> embed = AASequenceEmbedding ( ... d_model = 20 , ... aa_names = CONFIG . encoding_aa_order , ... mod_names = CONFIG . encoding_mod_order , ... max_length = 100 ,) >>> aa_embed , mod_embed = embed . as_DataFrames () >>> list ( aa_embed ) ['__missing__', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'c_term', 'n_term'] >>> list ( mod_embed ) [None, '[UNIMOD:4]', '[UNIMOD:35]', '[UNIMOD:21]', '[UNIMOD:121]', '[UNIMOD:737]', '[UNIMOD:1]', '[UNIMOD:34]', '[UNIMOD:36]', '[UNIMOD:37]', '[UNIMOD:354]', '[UNIMOD:7]', '__unknown1__'] Source code in elfragmentador/model/nn_encoding.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def as_DataFrames ( self ) -> Tuple [ DataFrame , DataFrame ]: \"\"\" Returns the weights as data frames. Returns: Tuple[DataFrame, DataFrame]: A data frame of the aminoacid embeddings and the modification embeddings Examples: >>> from elfragmentador.config import CONFIG >>> embed = AASequenceEmbedding( ... d_model=20, ... aa_names=CONFIG.encoding_aa_order, ... mod_names=CONFIG.encoding_mod_order, ... max_length=100,) >>> aa_embed, mod_embed = embed.as_DataFrames() >>> list(aa_embed) ['__missing__', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'c_term', 'n_term'] >>> list(mod_embed) [None, '[UNIMOD:4]', '[UNIMOD:35]', '[UNIMOD:21]', '[UNIMOD:121]', '[UNIMOD:737]', '[UNIMOD:1]', '[UNIMOD:34]', '[UNIMOD:36]', '[UNIMOD:37]', '[UNIMOD:354]', '[UNIMOD:7]', '__unknown1__'] \"\"\" df_aa = pd . DataFrame ( data = self . aa_encoder . weight . detach () . numpy () . T ) df_aa . columns = self . aa_names df_mod = pd . DataFrame ( data = self . mod_encoder . weight . detach () . cpu () . numpy () . T ) df_mod . columns = self . mod_names return df_aa , df_mod","title":"as_DataFrames()"},{"location":"CLI%20reference/cli/","text":"Usage of the multiple command line interface options ... ! elfragmentador append_pin -- help output usage: elfragmentador append_pin [-h] [--pin PIN] [--nce NCE] [--rawfile_locations RAWFILE_LOCATIONS] [--out OUT] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --pin PIN Input percolator file --nce NCE Collision energy to use for the prediction --rawfile_locations RAWFILE_LOCATIONS Locations to look for the raw files --out OUT Input percolator file --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference Evaluating prediction data with your own data! ! elfragmentador evaluate -- help output usage: elfragmentador evaluate [-h] [--input INPUT] [--nce NCE] [--out OUT] [--assure_notrain ASSURE_NOTRAIN] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --input INPUT Path to a file to use as a reference for the evaluation (.sptxt generally) --nce NCE Comma delimited series of collision energies to use --out OUT csv file to output results to --assure_notrain ASSURE_NOTRAIN Whether to remove all sequences that could be assigned to the training set --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference Example You can use several spectral library formats for compare the predictions from ElFragmentador with your data. In this case we will use the .peptides.txt file that mokapor uses as a default output! This also requires having the .mzML with the spectra in the same directory. (if it is not there it will try to find them in a couple of other directories). This will go over the different nces provided, find the one that matches the best the data provided (the first couple hundred spectra). Then it will use that that nce to predict all spectra in the file and compare them to the real one. It finally shows some \"plots\" on the performance and a csv file with the calculated metrics. Note that the --assure_notrain flag can be used to ignore in the similarity calculations all peptides that even had a chance to be in the training of the model. $ poetry run elfragmentador evaluate --input mokapot.peptides.txt.evaluation.log --nce 24,28,30,32,34,38,42 --out evaluation.csv --assure_notrain 1 Global seed set to 2020 2022-11-21 07:42:50.131 | INFO | elfragmentador.cli:greeting:72 - ElFragmentador version: 0.55.0a1 2022-11-21 07:42:50.131 | INFO | elfragmentador.cli:setup_model:39 - Loading model from https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt 2022-11-21 07:42:50.159 | INFO | elfragmentador.model:__init__:126 - num_decoder_layers 6 num_encoder_layers 5 nhid 120 d_model 64 nhead 4 dropout 0.02combined embeds True combined encoders False 2022-11-21 07:42:50.159 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-21 07:42:50.164 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-21 07:42:50.167 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=6 2022-11-21 07:42:50.171 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 174 2022-11-21 07:42:50.172 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-21 07:42:50.172 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-21 07:42:50.175 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=5 2022-11-21 07:42:50.180 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 1 2022-11-21 07:42:50.804 | INFO | elfragmentador.model:evaluate_landmark_rt:769 - {'polynomial': [0.010075807176896115, -0.0008703060814287248], 'determination': 0.9961272999162974} 2022-11-21 07:42:50.820 | INFO | elfragmentador.model:evaluate_landmark_rt:775 - Prediction vs real iRT of biognosys and procal peptides \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2597\u2596\u2590\u2502 1 \u2502 \u2502 \u2597\u2596 \u2597 \u259d\u2502 \u2502 \u2502 \u2596\u259e \u2580 \u2502 \u2502 \u2502 \u2584\u2584\u259a\u2596 \u2502 \u2502 \u2502 \u2584\u259d\u2596 \u2598 \u2502 \u2502 \u2502 \u2597\u2599\u259d\u259d \u2502 \u2502 \u2502 \u2596\u259d\u2598\u2598 \u2502 \u2502 \u2502 \u259e\u259d \u2598 \u2502 \u2502 \u2502 \u259f\u2597\u2598 \u2502 \u2502 \u2502 \u2596 \u2590\u2597 \u2502 \u2502 \u2502 \u259d\u2590 \u2502 \u2502 \u2502 \u2597\u2584\u259d\u259d\u259d \u2502 \u2502\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2597\u2581\u259a\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2596 \u259e \u2502 \u2502 \u2502 \u2597 \u2502 \u2502 \u2502 \u2598 \u2502 \u2502 \u2502\u2596\u2597 \u2598 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 -20 0 20 40 60 80 100 2022-11-21 07:42:50.821 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:42:55.007 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:03.947 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:03.953 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:07.883 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:16.347 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:16.349 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:20.260 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:28.969 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:28.977 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:32.877 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:41.633 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:41.647 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:45.604 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:54.175 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:54.190 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:58.125 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:44:06.524 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:44:06.526 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:44:10.412 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:44:18.825 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:44:18.827 | INFO | elfragmentador.data.predictor:screen_nce:74 - Best NCE: 30.0, with median spectral angle: 0.4119728918060316 2022-11-21 07:44:18.827 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:44:22.865 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3618/3618 [00:13<00:00, 273.69it/s] 2022-11-21 07:44:36.086 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 2790/3618 spectra Pred RT (y) vs RT (x) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2597\u259b\u259b\u2598 \u259d\u2502 \u2502 \u2588\u2599 \u2596\u2502 6,000 \u2502 \u2584 \u2597\u259f\u2598\u259d \u2502 \u2502 \u2597 \u259f\u259e\u2590\u2599\u259c\u2588\u259b\u2598 \u2502 \u2502 \u2597\u2596\u2590\u2584\u259f\u2588\u2588\u259b\u259b\u259b\u2598\u259d\u259d \u2502 \u2502 \u259e \u2584\u2584\u259d\u2599\u2588\u259e\u2588\u2588\u2590 \u2596 \u2502 4,000 \u2502 \u2596 \u2597 \u2590\u259f\u2584\u259f\u2588\u259e\u258c\u2588\u2588 \u2598 \u2502 \u2502 \u2598 \u2597\u2584\u258c\u259b\u2588\u2588\u2588\u2588\u2580\u259b\u2580 \u2502 \u2502 \u259d \u259e\u2590\u259f\u2588\u259b\u259f\u2588\u259b\u259b\u259b\u2596 \u2598 \u2502 2,000 \u2502 \u2597\u2596 \u259d\u2599\u2588\u2590\u2588\u2588\u2588\u259b\u259b\u2598\u259d\u259d \u2502 \u2502 \u2596 \u2596 \u2584\u2596\u2588\u2597\u2588\u2588\u259b\u259c\u2580\u2598 \u2502 \u2502\u2581\u2581\u2597\u2581\u2596\u2581\u2581\u2596\u2581\u2581\u2597\u2599\u2596\u2588\u2588\u2588\u2588\u2588\u2588\u2580\u258c\u259d\u2598\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2597\u2596\u2596 \u259f\u2584\u259e\u259f\u2588\u2588\u2588\u2580\u2580\u259d\u2598\u2598 \u2502 \u2502 \u258c\u2599\u2580\u259c\u2597\u2588\u259f\u259b\u259b\u2580\u2596 \u2597 \u259d \u2502 \u2502 \u2590\u2599\u2588\u2588\u2588\u259c\u2598\u2598 \u2598 \u259d \u2502 \u2502\u2580\u259d\u259b\u2598 \u2502 -2,000 \u2502 \u2597 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1,000 2,000 3,000 4,000 5,000 Histogram of the spectral angles Median: 0.33 Q1: 0.22 Q3: 0.47 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u259b\u2580\u2580\u2580\u2580\u2580\u258c \u2502 80 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2580\u2580\u2580\u258c \u2502 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2580\u2580\u2580\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u259b\u2580\u2580\u2598 \u2590\u2584\u2584\u2584 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 40 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502\u2584\u2584\u2584\u258c \u2590\u2584\u2584\u2584 \u2502 \u2502\u2584\u2584\u2584\u258c\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2590\u2584\u2584\u2584\u2584\u2584\u2584\u2502 0 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 0.2 0.4 0.6 0.8 Histogram of the spectral angles of only the fragment ions Median: 0.85 Q1: 0.79 Q3: 0.90 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2590\u2580\u2580\u259c \u2502 \u2502 \u2597\u2584\u2584\u259f \u2590 \u2502 200 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u259d\u2580\u2580\u259c \u2502 \u2502 \u2597\u2584\u2584\u259f \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 100 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590\u2580\u2580\u2580 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2584\u2584\u2584\u2584\u2584\u259f\u2580\u2580\u2580 \u2590 \u2502 \u2502\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u259b\u2580\u2580\u2599\u2584\u2584\u2584\u2584\u2584\u258c\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2590\u2584\u2584\u2584\u2584\u2584\u2584\u2502 0 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0.2 0.5 0.8 1 Predicting peptides directly from a fasta file To get the help for the function run ... ! elfragmentador predict -- help output usage: elfragmentador predict [-h] [--fasta FASTA] [--enzyme ENZYME] [--nce NCE] [--charges CHARGES] [--missed_cleavages MISSED_CLEAVAGES] [--min_length MIN_LENGTH] [--min_mz MIN_MZ] [--max_mz MAX_MZ] [--out OUT] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --fasta FASTA Input fasta file --enzyme ENZYME Enzyme to use to digest the fasta file --nce NCE Collision energy to use for the prediction --charges CHARGES Comma delimited series of charges to use --missed_cleavages MISSED_CLEAVAGES Maximum number of missed clevages --min_length MIN_LENGTH Minimum peptide length to consider --min_mz MIN_MZ Minimum precursor mz to use --max_mz MAX_MZ Maximum precursor mz to use --out OUT Output .dlib file --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference Example $ elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 7 --out foo.dlib Global seed set to 2020 2022-11-26 21:39:39.072 | INFO | elfragmentador.cli:greeting:72 - ElFragmentador version: 0.55.0a1 2022-11-26 21:39:39.072 | INFO | elfragmentador.cli:setup_model:39 - Loading model from https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt 2022-11-26 21:39:39.092 | INFO | elfragmentador.model:__init__:126 - num_decoder_layers 6 num_encoder_layers 5 nhid 120 d_model 64 nhead 4 dropout 0.02combined embeds True combined encoders False 2022-11-26 21:39:39.092 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-26 21:39:39.104 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-26 21:39:39.107 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=6 2022-11-26 21:39:39.110 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 174 2022-11-26 21:39:39.111 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-26 21:39:39.111 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-26 21:39:39.113 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=5 2022-11-26 21:39:39.115 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 1 2022-11-26 21:39:39.797 | INFO | elfragmentador.model:evaluate_landmark_rt:769 - {'polynomial': [0.010075807176896115, -0.0008703060814287248], 'determination': 0.9961272999162974} 2022-11-26 21:39:39.813 | INFO | elfragmentador.model:evaluate_landmark_rt:775 - Prediction vs real iRT of biognosys and procal peptides \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2597\u2596\u2590\u2502 1 \u2502 \u2502 \u2597\u2596 \u2597 \u259d\u2502 \u2502 \u2502 \u2596\u259e \u2580 \u2502 \u2502 \u2502 \u2584\u2584\u259a\u2596 \u2502 \u2502 \u2502 \u2584\u259d\u2596 \u2598 \u2502 \u2502 \u2502 \u2597\u2599\u259d\u259d \u2502 \u2502 \u2502 \u2596\u259d\u2598\u2598 \u2502 \u2502 \u2502 \u259e\u259d \u2598 \u2502 \u2502 \u2502 \u259f\u2597\u2598 \u2502 \u2502 \u2502 \u2596 \u2590\u2597 \u2502 \u2502 \u2502 \u259d\u2590 \u2502 \u2502 \u2502 \u2597\u2584\u259d\u259d\u259d \u2502 \u2502\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2597\u2581\u259a\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2596 \u259e \u2502 \u2502 \u2502 \u2597 \u2502 \u2502 \u2502 \u2598 \u2502 \u2502 \u2502\u2596\u2597 \u2598 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 -20 0 20 40 60 80 100 2022-11-26 21:39:39.816 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta using <class 'ms2ml.data.adapters.fasta.FastaAdapter'> 2022-11-26 21:39:39.816 | INFO | ms2ml.data.parsing.fasta:parse_file:52 - Processing file tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta, with enzyme=trypsin, missed_cleavages=0 min_length=7 max_length=30 2022-11-26 21:39:39.823 | INFO | ms2ml.data.parsing.fasta:parse_file:82 - Done, 468 sequences 2022-11-26 21:39:39.823 | INFO | ms2ml.data.parsing.fasta:parse_file:84 - Removed 205 duplicates 2022-11-26 21:39:39.823 | INFO | ms2ml.data.adapters.fasta:parse:86 - Number of peptides: 468 2022-11-26 21:39:39.824 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:228 - Setting up the adapter to keep training spectra 0%| | 0/468 [00:00<?, ?it/s]2022-11-26 21:39:39.831 | INFO | ms2ml.data.parsing.fasta:parse_file:52 - Processing file tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta, with enzyme=trypsin, missed_cleavages=0 min_length=7 max_length=30 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 457/468 [00:04<00:00, 114.90it/s]2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.fasta:parse_file:82 - Done, 468 sequences 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.fasta:parse_file:84 - Removed 205 duplicates 2022-11-26 21:39:44.393 | INFO | ms2ml.data.adapters.fasta:parse:86 - Number of peptides: 468 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 468/468 [00:04<00:00, 102.56it/s] 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.encyclopedia:write_encyclopedia:182 - Finished writing EncyclopeDIA database to foo.dlib 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.encyclopedia:write_encyclopedia:183 - Wrote 468 spectra ! elfragmentador train -- help output usage: elfragmentador train [-h] [--run_name RUN_NAME] [--wandb_project WANDB_PROJECT] [--terminator_patience TERMINATOR_PATIENCE] [--from_checkpoint FROM_CHECKPOINT] [--num_queries NUM_QUERIES] [--num_decoder_layers NUM_DECODER_LAYERS] [--num_encoder_layers NUM_ENCODER_LAYERS] [--nhid NHID] [--d_model D_MODEL] [--nhead NHEAD] [--dropout DROPOUT] [--combine_embeds | --no-combine_embeds] [--combine_encoders | --no-combine_encoders] [--final_decoder FINAL_DECODER] [--lr LR] [--scheduler SCHEDULER] [--lr_ratio LR_RATIO] [--loss_ratio LOSS_RATIO] [--batch_size BATCH_SIZE] [--data_dir DATA_DIR] [--logger [LOGGER]] [--enable_checkpointing [ENABLE_CHECKPOINTING]] [--default_root_dir DEFAULT_ROOT_DIR] [--gradient_clip_val GRADIENT_CLIP_VAL] [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM] [--num_nodes NUM_NODES] [--num_processes NUM_PROCESSES] [--devices DEVICES] [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]] [--tpu_cores TPU_CORES] [--ipus IPUS] [--enable_progress_bar [ENABLE_PROGRESS_BAR]] [--overfit_batches OVERFIT_BATCHES] [--track_grad_norm TRACK_GRAD_NORM] [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH] [--fast_dev_run [FAST_DEV_RUN]] [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES] [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS] [--max_steps MAX_STEPS] [--min_steps MIN_STEPS] [--max_time MAX_TIME] [--limit_train_batches LIMIT_TRAIN_BATCHES] [--limit_val_batches LIMIT_VAL_BATCHES] [--limit_test_batches LIMIT_TEST_BATCHES] [--limit_predict_batches LIMIT_PREDICT_BATCHES] [--val_check_interval VAL_CHECK_INTERVAL] [--log_every_n_steps LOG_EVERY_N_STEPS] [--accelerator ACCELERATOR] [--strategy STRATEGY] [--sync_batchnorm [SYNC_BATCHNORM]] [--precision PRECISION] [--enable_model_summary [ENABLE_MODEL_SUMMARY]] [--num_sanity_val_steps NUM_SANITY_VAL_STEPS] [--resume_from_checkpoint RESUME_FROM_CHECKPOINT] [--profiler PROFILER] [--benchmark [BENCHMARK]] [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS] [--auto_lr_find [AUTO_LR_FIND]] [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]] [--detect_anomaly [DETECT_ANOMALY]] [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]] [--plugins PLUGINS] [--amp_backend AMP_BACKEND] [--amp_level AMP_LEVEL] [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]] [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE] [--inference_mode [INFERENCE_MODE]] options: -h, --help show this help message and exit Program Parameters: Program level parameters, these should not change the outcome of the run --run_name RUN_NAME Name to be given to the run (logging) --wandb_project WANDB_PROJECT Wandb project to log to, check out wandb... please Model Parameters: Parameters that modify the model or its training (learn rate, scheduler, layers, dimension ...) --num_queries NUM_QUERIES Expected encoding length of the spectra --num_decoder_layers NUM_DECODER_LAYERS Number of sub-encoder-layers in the encoder --num_encoder_layers NUM_ENCODER_LAYERS Number of sub-encoder-layers in the decoder --nhid NHID Dimension of the feedforward networks --d_model D_MODEL Number of input features to the transformer encoder --nhead NHEAD Number of attention heads --dropout DROPOUT --combine_embeds, --no-combine_embeds Whether the embeddings for aminoacid and modifications should be shared between the irt and fragment sections --combine_encoders, --no-combine_encoders Whether the encoders for aminoacid and modifications should be shared between the irt and fragment sections --final_decoder FINAL_DECODER What kind of final layer should the docer have to output a single number, options are 'mlp' and 'linear' --lr LR --scheduler SCHEDULER Scheduler to use during training, either of ['plateau', 'cosine', 'onecycle'] --lr_ratio LR_RATIO For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1 For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn ratewould be 1.0 --loss_ratio LOSS_RATIO Ratio between the retention time and the spectrum loss (higher values mean more weight to the spectra loss with respect to the retention time loss) Data Parameters: Parameters for the loading of data --batch_size BATCH_SIZE --data_dir DATA_DIR Trainer Parameters: Parameters that modify the model or its training --terminator_patience TERMINATOR_PATIENCE Patience for early termination --from_checkpoint FROM_CHECKPOINT The path of a checkpoint to copy weights from before training pl.Trainer: --logger [LOGGER] Logger (or iterable collection of loggers) for experiment tracking. A True value uses the default TensorBoardLogger if it is installed, otherwise CSVLogger . False will disable logging. If multiple loggers are provided, local files (checkpoints, profiler traces, etc.) are saved in the log_dir of he first logger. Default: True . --enable_checkpointing [ENABLE_CHECKPOINTING] If True , enable checkpointing. It will configure a default ModelCheckpoint callback if there is no user- defined ModelCheckpoint in :paramref: ~pytorch_lightni ng.trainer.trainer.Trainer.callbacks . Default: True . --default_root_dir DEFAULT_ROOT_DIR Default path for logs and weights when no logger/ckpt_callback passed. Default: os.getcwd() . Can be remote file paths such as s3://mybucket/path or 'hdfs://path/' --gradient_clip_val GRADIENT_CLIP_VAL The value at which to clip gradients. Passing gradient_clip_val=None disables gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before. Default: None . --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. By default it will be set to \"norm\" . --num_nodes NUM_NODES Number of GPU nodes for distributed training. Default: 1 . --num_processes NUM_PROCESSES Number of processes for distributed training with accelerator=\"cpu\" . Default: 1 . .. deprecated:: v1.7 num_processes has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='cpu' and devices=x instead. --devices DEVICES Will be mapped to either gpus , tpu_cores , num_processes or ipus , based on the accelerator type. --gpus GPUS Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node Default: None . .. deprecated:: v1.7 gpus has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='gpu' and devices=x instead. --auto_select_gpus [AUTO_SELECT_GPUS] If enabled and gpus or devices is an integer, pick available gpus automatically. This is especially useful when GPUs are configured to be in \"exclusive mode\", such that only one process at a time can access them. Default: False . .. deprecated:: v1.9 auto_select_gpus has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function :fu nc: ~lightning_fabric.accelerators.cuda.find_usable_cu da_devices instead. --tpu_cores TPU_CORES How many TPU cores to train on (1 or 8) / Single TPU to train on (1) Default: None . .. deprecated:: v1.7 tpu_cores has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='tpu' and devices=x instead. --ipus IPUS How many IPUs to train on. Default: None . .. deprecated:: v1.7 ipus has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='ipu' and devices=x instead. --enable_progress_bar [ENABLE_PROGRESS_BAR] Whether to enable to progress bar by default. Default: True . --overfit_batches OVERFIT_BATCHES Overfit a fraction of training/validation data (float) or a set number of batches (int). Default: 0.0 . --track_grad_norm TRACK_GRAD_NORM -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them. Default: -1 . --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH Perform a validation loop every after every N training epochs. If None , validation will be done solely based on the number of training batches, requiring val_check_interval to be an integer value. Default: 1 . --fast_dev_run [FAST_DEV_RUN] Runs n if set to n (int) else 1 if set to True batch(es) of train, val and test to find any bugs (ie: a sort of unit test). Default: False . --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES Accumulates grads every k batches or as set up in the dict. Default: None . --max_epochs MAX_EPOCHS Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000 . To enable infinite training, set max_epochs = -1 . --min_epochs MIN_EPOCHS Force training for at least these many epochs. Disabled by default (None). --max_steps MAX_STEPS Stop training after this number of steps. Disabled by default (-1). If max_steps = -1 and max_epochs = None , will default to max_epochs = 1000 . To enable infinite training, set max_epochs to -1 . --min_steps MIN_STEPS Force training for at least these number of steps. Disabled by default ( None ). --max_time MAX_TIME Stop training after this amount of time has passed. Disabled by default ( None ). The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a :class: datetime.timedelta , or a dictionary with keys that will be passed to :class: datetime.timedelta . --limit_train_batches LIMIT_TRAIN_BATCHES How much of training dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_val_batches LIMIT_VAL_BATCHES How much of validation dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_test_batches LIMIT_TEST_BATCHES How much of test dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_predict_batches LIMIT_PREDICT_BATCHES How much of prediction dataset to check (float = fraction, int = num_batches). Default: 1.0 . --val_check_interval VAL_CHECK_INTERVAL How often to check the validation set. Pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch. Pass an int to check after a fixed number of training batches. An int value can only be higher than the number of training batches when check_val_every_n_epoch=None , which validates after every N training batches across epochs or during iteration-based training. Default: 1.0 . --log_every_n_steps LOG_EVERY_N_STEPS How often to log within steps. Default: 50 . --accelerator ACCELERATOR Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\") as well as custom accelerator instances. --strategy STRATEGY Supports different training strategies with aliases as well custom strategies. Default: None . --sync_batchnorm [SYNC_BATCHNORM] Synchronize batch norm layers between process groups/whole world. Default: False . --precision PRECISION Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16). Can be used on CPU, GPU, TPUs, HPUs or IPUs. Default: 32 . --enable_model_summary [ENABLE_MODEL_SUMMARY] Whether to enable model summarization by default. Default: True . --num_sanity_val_steps NUM_SANITY_VAL_STEPS Sanity check runs n validation batches before starting the training routine. Set it to -1 to run all batches in all validation dataloaders. Default: 2 . --resume_from_checkpoint RESUME_FROM_CHECKPOINT Path/URL of the checkpoint from which training is resumed. If there is no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint, training will start from the beginning of the next epoch. .. deprecated:: v1.5 resume_from_checkpoint is deprecated in v1.5 and will be removed in v2.0. Please pass the path to Trainer.fit(..., ckpt_path=...) instead. --profiler PROFILER To profile individual steps during training and assist in identifying bottlenecks. Default: None . --benchmark [BENCHMARK] The value ( True or False ) to set torch.backends.cudnn.benchmark to. The value for torch.backends.cudnn.benchmark set in the current session will be used ( False if not manually set). If :paramref: ~pytorch_lightning.trainer.Trainer.deter ministic is set to True , this will default to False . Override to manually set a different value. Default: None . --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS Set to a non-negative integer to reload dataloaders every n epochs. Default: 0 . --auto_lr_find [AUTO_LR_FIND] If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence. trainer.tune() method will set the suggested learning rate in self.lr or self.learning_rate in the LightningModule. To use a different key set a string instead of True with the key name. Default: False . --replace_sampler_ddp [REPLACE_SAMPLER_DDP] Explicitly enables or disables sampler replacement. If not specified this will toggled automatically when DDP is used. By default it will add shuffle=True for train sampler and shuffle=False for val/test sampler. If you want to customize it, you can set replace_sampler_ddp=False and add your own distributed sampler. --detect_anomaly [DETECT_ANOMALY] Enable anomaly detection for the autograd engine. Default: False . --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE] If set to True, will initially run a batch size finder trying to find the largest batch size that fits into memory. The result will be stored in self.batch_size in the LightningModule or LightningDataModule depending on your setup. Additionally, can be set to either power that estimates the batch size through a power search or binsearch that estimates the batch size through a binary search. Default: False . --plugins PLUGINS Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins. Default: None . --amp_backend AMP_BACKEND The mixed precision backend to use (\"native\" or \"apex\"). Default: 'native'' . .. deprecated:: v1.9 Setting amp_backend inside the Trainer is deprecated in v1.8.0 and will be removed in v2.0.0. This argument was only relevant for apex which is being removed. --amp_level AMP_LEVEL The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\" if amp_backend is set to \"apex\". .. deprecated:: v1.8 Setting amp_level inside the Trainer is deprecated in v1.8.0 and will be removed in v2.0.0. --move_metrics_to_cpu [MOVE_METRICS_TO_CPU] Whether to force internal logged metrics to be moved to cpu. This can save some gpu memory, but can make training slower. Use with attention. Default: False . --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE How to loop over the datasets when there are multiple train loaders. In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed, and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets reload when reaching the minimum length of datasets. Default: \"max_size_cycle\" . --inference_mode [INFERENCE_MODE] Whether to use :func: torch.inference_mode or :func: torch.no_grad during evaluation ( validate / test / predict ).","title":"Usage of the multiple command line interface options ..."},{"location":"CLI%20reference/cli/#usage-of-the-multiple-command-line-interface-options","text":"! elfragmentador append_pin -- help output usage: elfragmentador append_pin [-h] [--pin PIN] [--nce NCE] [--rawfile_locations RAWFILE_LOCATIONS] [--out OUT] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --pin PIN Input percolator file --nce NCE Collision energy to use for the prediction --rawfile_locations RAWFILE_LOCATIONS Locations to look for the raw files --out OUT Input percolator file --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference","title":"Usage of the multiple command line interface options ..."},{"location":"CLI%20reference/cli/#evaluating-prediction-data-with-your-own-data","text":"! elfragmentador evaluate -- help output usage: elfragmentador evaluate [-h] [--input INPUT] [--nce NCE] [--out OUT] [--assure_notrain ASSURE_NOTRAIN] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --input INPUT Path to a file to use as a reference for the evaluation (.sptxt generally) --nce NCE Comma delimited series of collision energies to use --out OUT csv file to output results to --assure_notrain ASSURE_NOTRAIN Whether to remove all sequences that could be assigned to the training set --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference","title":"Evaluating prediction data with your own data!"},{"location":"CLI%20reference/cli/#example","text":"You can use several spectral library formats for compare the predictions from ElFragmentador with your data. In this case we will use the .peptides.txt file that mokapor uses as a default output! This also requires having the .mzML with the spectra in the same directory. (if it is not there it will try to find them in a couple of other directories). This will go over the different nces provided, find the one that matches the best the data provided (the first couple hundred spectra). Then it will use that that nce to predict all spectra in the file and compare them to the real one. It finally shows some \"plots\" on the performance and a csv file with the calculated metrics. Note that the --assure_notrain flag can be used to ignore in the similarity calculations all peptides that even had a chance to be in the training of the model. $ poetry run elfragmentador evaluate --input mokapot.peptides.txt.evaluation.log --nce 24,28,30,32,34,38,42 --out evaluation.csv --assure_notrain 1 Global seed set to 2020 2022-11-21 07:42:50.131 | INFO | elfragmentador.cli:greeting:72 - ElFragmentador version: 0.55.0a1 2022-11-21 07:42:50.131 | INFO | elfragmentador.cli:setup_model:39 - Loading model from https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt 2022-11-21 07:42:50.159 | INFO | elfragmentador.model:__init__:126 - num_decoder_layers 6 num_encoder_layers 5 nhid 120 d_model 64 nhead 4 dropout 0.02combined embeds True combined encoders False 2022-11-21 07:42:50.159 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-21 07:42:50.164 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-21 07:42:50.167 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=6 2022-11-21 07:42:50.171 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 174 2022-11-21 07:42:50.172 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-21 07:42:50.172 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-21 07:42:50.175 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=5 2022-11-21 07:42:50.180 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 1 2022-11-21 07:42:50.804 | INFO | elfragmentador.model:evaluate_landmark_rt:769 - {'polynomial': [0.010075807176896115, -0.0008703060814287248], 'determination': 0.9961272999162974} 2022-11-21 07:42:50.820 | INFO | elfragmentador.model:evaluate_landmark_rt:775 - Prediction vs real iRT of biognosys and procal peptides \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2597\u2596\u2590\u2502 1 \u2502 \u2502 \u2597\u2596 \u2597 \u259d\u2502 \u2502 \u2502 \u2596\u259e \u2580 \u2502 \u2502 \u2502 \u2584\u2584\u259a\u2596 \u2502 \u2502 \u2502 \u2584\u259d\u2596 \u2598 \u2502 \u2502 \u2502 \u2597\u2599\u259d\u259d \u2502 \u2502 \u2502 \u2596\u259d\u2598\u2598 \u2502 \u2502 \u2502 \u259e\u259d \u2598 \u2502 \u2502 \u2502 \u259f\u2597\u2598 \u2502 \u2502 \u2502 \u2596 \u2590\u2597 \u2502 \u2502 \u2502 \u259d\u2590 \u2502 \u2502 \u2502 \u2597\u2584\u259d\u259d\u259d \u2502 \u2502\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2597\u2581\u259a\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2596 \u259e \u2502 \u2502 \u2502 \u2597 \u2502 \u2502 \u2502 \u2598 \u2502 \u2502 \u2502\u2596\u2597 \u2598 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 -20 0 20 40 60 80 100 2022-11-21 07:42:50.821 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:42:55.007 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:03.947 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:03.953 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:07.883 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:16.347 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:16.349 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:20.260 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:28.969 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:28.977 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:32.877 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:41.633 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:41.647 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:45.604 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:43:54.175 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:43:54.190 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:43:58.125 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:44:06.524 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:44:06.526 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:44:10.412 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 2022-11-21 07:44:18.825 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 1955/2456 spectra 2022-11-21 07:44:18.827 | INFO | elfragmentador.data.predictor:screen_nce:74 - Best NCE: 30.0, with median spectral angle: 0.4119728918060316 2022-11-21 07:44:18.827 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from evaluation/results/CElegansGG/mokapot/mokapot.peptides.txt using <class 'ms2ml.data.adapters.mokapot.MokapotPSMAdapter'> 2022-11-21 07:44:22.865 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:218 - Setting up the adapter to drop training spectra 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3618/3618 [00:13<00:00, 273.69it/s] 2022-11-21 07:44:36.086 | INFO | elfragmentador.data.predictor:compare:119 - Skipped 2790/3618 spectra Pred RT (y) vs RT (x) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2597\u259b\u259b\u2598 \u259d\u2502 \u2502 \u2588\u2599 \u2596\u2502 6,000 \u2502 \u2584 \u2597\u259f\u2598\u259d \u2502 \u2502 \u2597 \u259f\u259e\u2590\u2599\u259c\u2588\u259b\u2598 \u2502 \u2502 \u2597\u2596\u2590\u2584\u259f\u2588\u2588\u259b\u259b\u259b\u2598\u259d\u259d \u2502 \u2502 \u259e \u2584\u2584\u259d\u2599\u2588\u259e\u2588\u2588\u2590 \u2596 \u2502 4,000 \u2502 \u2596 \u2597 \u2590\u259f\u2584\u259f\u2588\u259e\u258c\u2588\u2588 \u2598 \u2502 \u2502 \u2598 \u2597\u2584\u258c\u259b\u2588\u2588\u2588\u2588\u2580\u259b\u2580 \u2502 \u2502 \u259d \u259e\u2590\u259f\u2588\u259b\u259f\u2588\u259b\u259b\u259b\u2596 \u2598 \u2502 2,000 \u2502 \u2597\u2596 \u259d\u2599\u2588\u2590\u2588\u2588\u2588\u259b\u259b\u2598\u259d\u259d \u2502 \u2502 \u2596 \u2596 \u2584\u2596\u2588\u2597\u2588\u2588\u259b\u259c\u2580\u2598 \u2502 \u2502\u2581\u2581\u2597\u2581\u2596\u2581\u2581\u2596\u2581\u2581\u2597\u2599\u2596\u2588\u2588\u2588\u2588\u2588\u2588\u2580\u258c\u259d\u2598\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2597\u2596\u2596 \u259f\u2584\u259e\u259f\u2588\u2588\u2588\u2580\u2580\u259d\u2598\u2598 \u2502 \u2502 \u258c\u2599\u2580\u259c\u2597\u2588\u259f\u259b\u259b\u2580\u2596 \u2597 \u259d \u2502 \u2502 \u2590\u2599\u2588\u2588\u2588\u259c\u2598\u2598 \u2598 \u259d \u2502 \u2502\u2580\u259d\u259b\u2598 \u2502 -2,000 \u2502 \u2597 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1,000 2,000 3,000 4,000 5,000 Histogram of the spectral angles Median: 0.33 Q1: 0.22 Q3: 0.47 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u259b\u2580\u2580\u2580\u2580\u2580\u258c \u2502 80 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2580\u2580\u2580\u258c \u2502 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2580\u2580\u2580\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u259b\u2580\u2580\u2598 \u2590\u2584\u2584\u2584 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 40 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502 \u258c \u2590 \u2502 \u2502 \u2502 \u2584\u2584\u2584\u258c \u2590 \u2502 \u2502 \u2502 \u258c \u259d\u2580\u2580\u259c \u2502 \u2502 \u2502\u2584\u2584\u2584\u258c \u2590\u2584\u2584\u2584 \u2502 \u2502\u2584\u2584\u2584\u258c\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2590\u2584\u2584\u2584\u2584\u2584\u2584\u2502 0 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0 0.2 0.4 0.6 0.8 Histogram of the spectral angles of only the fragment ions Median: 0.85 Q1: 0.79 Q3: 0.90 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2590\u2580\u2580\u259c \u2502 \u2502 \u2597\u2584\u2584\u259f \u2590 \u2502 200 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u259d\u2580\u2580\u259c \u2502 \u2502 \u2597\u2584\u2584\u259f \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 100 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590\u2580\u2580\u2580 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2590 \u2590 \u2502 \u2502 \u2584\u2584\u2584\u2584\u2584\u259f\u2580\u2580\u2580 \u2590 \u2502 \u2502\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u259b\u2580\u2580\u2599\u2584\u2584\u2584\u2584\u2584\u258c\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2590\u2584\u2584\u2584\u2584\u2584\u2584\u2502 0 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 0.2 0.5 0.8 1","title":"Example"},{"location":"CLI%20reference/cli/#predicting-peptides-directly-from-a-fasta-file","text":"To get the help for the function run ... ! elfragmentador predict -- help output usage: elfragmentador predict [-h] [--fasta FASTA] [--enzyme ENZYME] [--nce NCE] [--charges CHARGES] [--missed_cleavages MISSED_CLEAVAGES] [--min_length MIN_LENGTH] [--min_mz MIN_MZ] [--max_mz MAX_MZ] [--out OUT] [--model_checkpoint MODEL_CHECKPOINT] [--threads THREADS] options: -h, --help show this help message and exit --fasta FASTA Input fasta file --enzyme ENZYME Enzyme to use to digest the fasta file --nce NCE Collision energy to use for the prediction --charges CHARGES Comma delimited series of charges to use --missed_cleavages MISSED_CLEAVAGES Maximum number of missed clevages --min_length MIN_LENGTH Minimum peptide length to consider --min_mz MIN_MZ Minimum precursor mz to use --max_mz MAX_MZ Maximum precursor mz to use --out OUT Output .dlib file --model_checkpoint MODEL_CHECKPOINT Model checkpoint to use for the prediction, if nothing is passed will download a pretrained model --threads THREADS Number of threads to use during inference","title":"Predicting peptides directly from a fasta file"},{"location":"CLI%20reference/cli/#example_1","text":"$ elfragmentador predict --fasta tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta --nce 32 --charges 2 --missed_cleavages 0 --min_length 7 --out foo.dlib Global seed set to 2020 2022-11-26 21:39:39.072 | INFO | elfragmentador.cli:greeting:72 - ElFragmentador version: 0.55.0a1 2022-11-26 21:39:39.072 | INFO | elfragmentador.cli:setup_model:39 - Loading model from https://github.com/jspaezp/elfragmentador-modelzoo/raw/9e6ee76cde441d2459ec52418ec6f874e69f9a7b/0.55.0a2/0.55.0a2_ef_21e_4h_64_120_nel5ndl6_500_lmlp_sim01_val_l%3D0.129946_epoch%3D020.ckpt 2022-11-26 21:39:39.092 | INFO | elfragmentador.model:__init__:126 - num_decoder_layers 6 num_encoder_layers 5 nhid 120 d_model 64 nhead 4 dropout 0.02combined embeds True combined encoders False 2022-11-26 21:39:39.092 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-26 21:39:39.104 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-26 21:39:39.107 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=6 2022-11-26 21:39:39.110 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 174 2022-11-26 21:39:39.111 | INFO | elfragmentador.model.nn_encoding:__init__:210 - Initializing AASequenceEmbedding 2022-11-26 21:39:39.111 | INFO | elfragmentador.model.nn_encoding:init_weights:243 - Initializing weights on AASequenceEmbedding 2022-11-26 21:39:39.113 | INFO | elfragmentador.model.transformer_layers:__init__:87 - Creating TransformerDecoder nhid=120, d_model=64 nhead=4 layers=5 2022-11-26 21:39:39.115 | INFO | elfragmentador.model.transformer_layers:__init__:109 - Creating embedding for spectra of length 1 2022-11-26 21:39:39.797 | INFO | elfragmentador.model:evaluate_landmark_rt:769 - {'polynomial': [0.010075807176896115, -0.0008703060814287248], 'determination': 0.9961272999162974} 2022-11-26 21:39:39.813 | INFO | elfragmentador.model:evaluate_landmark_rt:775 - Prediction vs real iRT of biognosys and procal peptides \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2597\u2596\u2590\u2502 1 \u2502 \u2502 \u2597\u2596 \u2597 \u259d\u2502 \u2502 \u2502 \u2596\u259e \u2580 \u2502 \u2502 \u2502 \u2584\u2584\u259a\u2596 \u2502 \u2502 \u2502 \u2584\u259d\u2596 \u2598 \u2502 \u2502 \u2502 \u2597\u2599\u259d\u259d \u2502 \u2502 \u2502 \u2596\u259d\u2598\u2598 \u2502 \u2502 \u2502 \u259e\u259d \u2598 \u2502 \u2502 \u2502 \u259f\u2597\u2598 \u2502 \u2502 \u2502 \u2596 \u2590\u2597 \u2502 \u2502 \u2502 \u259d\u2590 \u2502 \u2502 \u2502 \u2597\u2584\u259d\u259d\u259d \u2502 \u2502\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2597\u2581\u259a\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2502 0 \u2502 \u2596 \u259e \u2502 \u2502 \u2502 \u2597 \u2502 \u2502 \u2502 \u2598 \u2502 \u2502 \u2502\u2596\u2597 \u2598 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 -20 0 20 40 60 80 100 2022-11-26 21:39:39.816 | INFO | ms2ml.data.adapters:read_data:52 - Reading data from tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta using <class 'ms2ml.data.adapters.fasta.FastaAdapter'> 2022-11-26 21:39:39.816 | INFO | ms2ml.data.parsing.fasta:parse_file:52 - Processing file tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta, with enzyme=trypsin, missed_cleavages=0 min_length=7 max_length=30 2022-11-26 21:39:39.823 | INFO | ms2ml.data.parsing.fasta:parse_file:82 - Done, 468 sequences 2022-11-26 21:39:39.823 | INFO | ms2ml.data.parsing.fasta:parse_file:84 - Removed 205 duplicates 2022-11-26 21:39:39.823 | INFO | ms2ml.data.adapters.fasta:parse:86 - Number of peptides: 468 2022-11-26 21:39:39.824 | INFO | elfragmentador.data.predictor:adapter_out_hook_predict_factory:228 - Setting up the adapter to keep training spectra 0%| | 0/468 [00:00<?, ?it/s]2022-11-26 21:39:39.831 | INFO | ms2ml.data.parsing.fasta:parse_file:52 - Processing file tests/data/fasta/uniprot-proteome_UP000464024_reviewed_yes.fasta, with enzyme=trypsin, missed_cleavages=0 min_length=7 max_length=30 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 457/468 [00:04<00:00, 114.90it/s]2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.fasta:parse_file:82 - Done, 468 sequences 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.fasta:parse_file:84 - Removed 205 duplicates 2022-11-26 21:39:44.393 | INFO | ms2ml.data.adapters.fasta:parse:86 - Number of peptides: 468 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 468/468 [00:04<00:00, 102.56it/s] 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.encyclopedia:write_encyclopedia:182 - Finished writing EncyclopeDIA database to foo.dlib 2022-11-26 21:39:44.393 | INFO | ms2ml.data.parsing.encyclopedia:write_encyclopedia:183 - Wrote 468 spectra ! elfragmentador train -- help output usage: elfragmentador train [-h] [--run_name RUN_NAME] [--wandb_project WANDB_PROJECT] [--terminator_patience TERMINATOR_PATIENCE] [--from_checkpoint FROM_CHECKPOINT] [--num_queries NUM_QUERIES] [--num_decoder_layers NUM_DECODER_LAYERS] [--num_encoder_layers NUM_ENCODER_LAYERS] [--nhid NHID] [--d_model D_MODEL] [--nhead NHEAD] [--dropout DROPOUT] [--combine_embeds | --no-combine_embeds] [--combine_encoders | --no-combine_encoders] [--final_decoder FINAL_DECODER] [--lr LR] [--scheduler SCHEDULER] [--lr_ratio LR_RATIO] [--loss_ratio LOSS_RATIO] [--batch_size BATCH_SIZE] [--data_dir DATA_DIR] [--logger [LOGGER]] [--enable_checkpointing [ENABLE_CHECKPOINTING]] [--default_root_dir DEFAULT_ROOT_DIR] [--gradient_clip_val GRADIENT_CLIP_VAL] [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM] [--num_nodes NUM_NODES] [--num_processes NUM_PROCESSES] [--devices DEVICES] [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]] [--tpu_cores TPU_CORES] [--ipus IPUS] [--enable_progress_bar [ENABLE_PROGRESS_BAR]] [--overfit_batches OVERFIT_BATCHES] [--track_grad_norm TRACK_GRAD_NORM] [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH] [--fast_dev_run [FAST_DEV_RUN]] [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES] [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS] [--max_steps MAX_STEPS] [--min_steps MIN_STEPS] [--max_time MAX_TIME] [--limit_train_batches LIMIT_TRAIN_BATCHES] [--limit_val_batches LIMIT_VAL_BATCHES] [--limit_test_batches LIMIT_TEST_BATCHES] [--limit_predict_batches LIMIT_PREDICT_BATCHES] [--val_check_interval VAL_CHECK_INTERVAL] [--log_every_n_steps LOG_EVERY_N_STEPS] [--accelerator ACCELERATOR] [--strategy STRATEGY] [--sync_batchnorm [SYNC_BATCHNORM]] [--precision PRECISION] [--enable_model_summary [ENABLE_MODEL_SUMMARY]] [--num_sanity_val_steps NUM_SANITY_VAL_STEPS] [--resume_from_checkpoint RESUME_FROM_CHECKPOINT] [--profiler PROFILER] [--benchmark [BENCHMARK]] [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS] [--auto_lr_find [AUTO_LR_FIND]] [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]] [--detect_anomaly [DETECT_ANOMALY]] [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]] [--plugins PLUGINS] [--amp_backend AMP_BACKEND] [--amp_level AMP_LEVEL] [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]] [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE] [--inference_mode [INFERENCE_MODE]] options: -h, --help show this help message and exit Program Parameters: Program level parameters, these should not change the outcome of the run --run_name RUN_NAME Name to be given to the run (logging) --wandb_project WANDB_PROJECT Wandb project to log to, check out wandb... please Model Parameters: Parameters that modify the model or its training (learn rate, scheduler, layers, dimension ...) --num_queries NUM_QUERIES Expected encoding length of the spectra --num_decoder_layers NUM_DECODER_LAYERS Number of sub-encoder-layers in the encoder --num_encoder_layers NUM_ENCODER_LAYERS Number of sub-encoder-layers in the decoder --nhid NHID Dimension of the feedforward networks --d_model D_MODEL Number of input features to the transformer encoder --nhead NHEAD Number of attention heads --dropout DROPOUT --combine_embeds, --no-combine_embeds Whether the embeddings for aminoacid and modifications should be shared between the irt and fragment sections --combine_encoders, --no-combine_encoders Whether the encoders for aminoacid and modifications should be shared between the irt and fragment sections --final_decoder FINAL_DECODER What kind of final layer should the docer have to output a single number, options are 'mlp' and 'linear' --lr LR --scheduler SCHEDULER Scheduler to use during training, either of ['plateau', 'cosine', 'onecycle'] --lr_ratio LR_RATIO For cosine annealing: Ratio of the initial learning rate to use with cosine annealing for instance a lr or 1 and a ratio of 10 would have a minimum learning rate of 0.1 For onecycle: Ratio of the initial lr and and maximum one, for instance if lr is 0.1 and ratio is 10, the max learn ratewould be 1.0 --loss_ratio LOSS_RATIO Ratio between the retention time and the spectrum loss (higher values mean more weight to the spectra loss with respect to the retention time loss) Data Parameters: Parameters for the loading of data --batch_size BATCH_SIZE --data_dir DATA_DIR Trainer Parameters: Parameters that modify the model or its training --terminator_patience TERMINATOR_PATIENCE Patience for early termination --from_checkpoint FROM_CHECKPOINT The path of a checkpoint to copy weights from before training pl.Trainer: --logger [LOGGER] Logger (or iterable collection of loggers) for experiment tracking. A True value uses the default TensorBoardLogger if it is installed, otherwise CSVLogger . False will disable logging. If multiple loggers are provided, local files (checkpoints, profiler traces, etc.) are saved in the log_dir of he first logger. Default: True . --enable_checkpointing [ENABLE_CHECKPOINTING] If True , enable checkpointing. It will configure a default ModelCheckpoint callback if there is no user- defined ModelCheckpoint in :paramref: ~pytorch_lightni ng.trainer.trainer.Trainer.callbacks . Default: True . --default_root_dir DEFAULT_ROOT_DIR Default path for logs and weights when no logger/ckpt_callback passed. Default: os.getcwd() . Can be remote file paths such as s3://mybucket/path or 'hdfs://path/' --gradient_clip_val GRADIENT_CLIP_VAL The value at which to clip gradients. Passing gradient_clip_val=None disables gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before. Default: None . --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. By default it will be set to \"norm\" . --num_nodes NUM_NODES Number of GPU nodes for distributed training. Default: 1 . --num_processes NUM_PROCESSES Number of processes for distributed training with accelerator=\"cpu\" . Default: 1 . .. deprecated:: v1.7 num_processes has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='cpu' and devices=x instead. --devices DEVICES Will be mapped to either gpus , tpu_cores , num_processes or ipus , based on the accelerator type. --gpus GPUS Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node Default: None . .. deprecated:: v1.7 gpus has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='gpu' and devices=x instead. --auto_select_gpus [AUTO_SELECT_GPUS] If enabled and gpus or devices is an integer, pick available gpus automatically. This is especially useful when GPUs are configured to be in \"exclusive mode\", such that only one process at a time can access them. Default: False . .. deprecated:: v1.9 auto_select_gpus has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function :fu nc: ~lightning_fabric.accelerators.cuda.find_usable_cu da_devices instead. --tpu_cores TPU_CORES How many TPU cores to train on (1 or 8) / Single TPU to train on (1) Default: None . .. deprecated:: v1.7 tpu_cores has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='tpu' and devices=x instead. --ipus IPUS How many IPUs to train on. Default: None . .. deprecated:: v1.7 ipus has been deprecated in v1.7 and will be removed in v2.0. Please use accelerator='ipu' and devices=x instead. --enable_progress_bar [ENABLE_PROGRESS_BAR] Whether to enable to progress bar by default. Default: True . --overfit_batches OVERFIT_BATCHES Overfit a fraction of training/validation data (float) or a set number of batches (int). Default: 0.0 . --track_grad_norm TRACK_GRAD_NORM -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them. Default: -1 . --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH Perform a validation loop every after every N training epochs. If None , validation will be done solely based on the number of training batches, requiring val_check_interval to be an integer value. Default: 1 . --fast_dev_run [FAST_DEV_RUN] Runs n if set to n (int) else 1 if set to True batch(es) of train, val and test to find any bugs (ie: a sort of unit test). Default: False . --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES Accumulates grads every k batches or as set up in the dict. Default: None . --max_epochs MAX_EPOCHS Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000 . To enable infinite training, set max_epochs = -1 . --min_epochs MIN_EPOCHS Force training for at least these many epochs. Disabled by default (None). --max_steps MAX_STEPS Stop training after this number of steps. Disabled by default (-1). If max_steps = -1 and max_epochs = None , will default to max_epochs = 1000 . To enable infinite training, set max_epochs to -1 . --min_steps MIN_STEPS Force training for at least these number of steps. Disabled by default ( None ). --max_time MAX_TIME Stop training after this amount of time has passed. Disabled by default ( None ). The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a :class: datetime.timedelta , or a dictionary with keys that will be passed to :class: datetime.timedelta . --limit_train_batches LIMIT_TRAIN_BATCHES How much of training dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_val_batches LIMIT_VAL_BATCHES How much of validation dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_test_batches LIMIT_TEST_BATCHES How much of test dataset to check (float = fraction, int = num_batches). Default: 1.0 . --limit_predict_batches LIMIT_PREDICT_BATCHES How much of prediction dataset to check (float = fraction, int = num_batches). Default: 1.0 . --val_check_interval VAL_CHECK_INTERVAL How often to check the validation set. Pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch. Pass an int to check after a fixed number of training batches. An int value can only be higher than the number of training batches when check_val_every_n_epoch=None , which validates after every N training batches across epochs or during iteration-based training. Default: 1.0 . --log_every_n_steps LOG_EVERY_N_STEPS How often to log within steps. Default: 50 . --accelerator ACCELERATOR Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\") as well as custom accelerator instances. --strategy STRATEGY Supports different training strategies with aliases as well custom strategies. Default: None . --sync_batchnorm [SYNC_BATCHNORM] Synchronize batch norm layers between process groups/whole world. Default: False . --precision PRECISION Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16). Can be used on CPU, GPU, TPUs, HPUs or IPUs. Default: 32 . --enable_model_summary [ENABLE_MODEL_SUMMARY] Whether to enable model summarization by default. Default: True . --num_sanity_val_steps NUM_SANITY_VAL_STEPS Sanity check runs n validation batches before starting the training routine. Set it to -1 to run all batches in all validation dataloaders. Default: 2 . --resume_from_checkpoint RESUME_FROM_CHECKPOINT Path/URL of the checkpoint from which training is resumed. If there is no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint, training will start from the beginning of the next epoch. .. deprecated:: v1.5 resume_from_checkpoint is deprecated in v1.5 and will be removed in v2.0. Please pass the path to Trainer.fit(..., ckpt_path=...) instead. --profiler PROFILER To profile individual steps during training and assist in identifying bottlenecks. Default: None . --benchmark [BENCHMARK] The value ( True or False ) to set torch.backends.cudnn.benchmark to. The value for torch.backends.cudnn.benchmark set in the current session will be used ( False if not manually set). If :paramref: ~pytorch_lightning.trainer.Trainer.deter ministic is set to True , this will default to False . Override to manually set a different value. Default: None . --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS Set to a non-negative integer to reload dataloaders every n epochs. Default: 0 . --auto_lr_find [AUTO_LR_FIND] If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence. trainer.tune() method will set the suggested learning rate in self.lr or self.learning_rate in the LightningModule. To use a different key set a string instead of True with the key name. Default: False . --replace_sampler_ddp [REPLACE_SAMPLER_DDP] Explicitly enables or disables sampler replacement. If not specified this will toggled automatically when DDP is used. By default it will add shuffle=True for train sampler and shuffle=False for val/test sampler. If you want to customize it, you can set replace_sampler_ddp=False and add your own distributed sampler. --detect_anomaly [DETECT_ANOMALY] Enable anomaly detection for the autograd engine. Default: False . --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE] If set to True, will initially run a batch size finder trying to find the largest batch size that fits into memory. The result will be stored in self.batch_size in the LightningModule or LightningDataModule depending on your setup. Additionally, can be set to either power that estimates the batch size through a power search or binsearch that estimates the batch size through a binary search. Default: False . --plugins PLUGINS Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins. Default: None . --amp_backend AMP_BACKEND The mixed precision backend to use (\"native\" or \"apex\"). Default: 'native'' . .. deprecated:: v1.9 Setting amp_backend inside the Trainer is deprecated in v1.8.0 and will be removed in v2.0.0. This argument was only relevant for apex which is being removed. --amp_level AMP_LEVEL The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\" if amp_backend is set to \"apex\". .. deprecated:: v1.8 Setting amp_level inside the Trainer is deprecated in v1.8.0 and will be removed in v2.0.0. --move_metrics_to_cpu [MOVE_METRICS_TO_CPU] Whether to force internal logged metrics to be moved to cpu. This can save some gpu memory, but can make training slower. Use with attention. Default: False . --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE How to loop over the datasets when there are multiple train loaders. In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed, and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets reload when reaching the minimum length of datasets. Default: \"max_size_cycle\" . --inference_mode [INFERENCE_MODE] Whether to use :func: torch.inference_mode or :func: torch.no_grad during evaluation ( validate / test / predict ).","title":"Example"}]}